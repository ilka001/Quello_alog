#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šçº¿ç¨‹HRVç‰¹å¾è®¡ç®—è„šæœ¬ - æ‰¹é‡å¤„ç†ç‰ˆæœ¬ (40ç§’)
æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªäººçš„æ•°æ®
è¾“å…¥æ ¼å¼ï¼š
1äººå
æƒ…ç»ªæ ‡ç­¾1 å¯¹åº”åˆ†æ•°
æƒ…ç»ªæ ‡ç­¾2 å¯¹åº”åˆ†æ•°
2äººå
æƒ…ç»ªæ ‡ç­¾1 å¯¹åº”åˆ†æ•°
...
"""

import os
import math
import glob
import numpy as np
import pandas as pd
from scipy import signal
from scipy.stats import entropy, zscore
from scipy.signal import find_peaks
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict
import time
import sys
import subprocess

# --------------------------------------------------------------------------------------------
# é…ç½®åŒºåŸŸ - ç”¨æˆ·éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†
# --------------------------------------------------------------------------------------------

# è¾“å…¥æ•°æ®ç›®å½•ï¼ˆåŒ…å«å·²åˆ’åˆ†å¥½çš„CSVæ–‡ä»¶ï¼‰
INPUT_DATA_DIR = r"C:\Users\UiNCeY\Desktop\emotion\processed_data"

# è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE = "hrv_data_40s.csv"

# RRé—´éš”æ•°æ®å¤‡ä»½ç›®å½•ï¼ˆæ¯ä¸ªæ ‡ç­¾å•ç‹¬ä¿å­˜ï¼‰
RR_BACKUP_BASE_DIR = r"C:\Users\UiNCeY\Desktop\emotion\40sdata\backup_batch"  # æ‰¹é‡ç‰ˆæœ¬å¤‡ä»½ç›®å½•

# æœ€å¤§çº¿ç¨‹æ•°ï¼ˆå»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„1-2å€ï¼Œå› ä¸ºHRVè®¡ç®—æ˜¯CPUå¯†é›†å‹ï¼‰
MAX_WORKERS = 14

# æ™ºèƒ½æ•°æ®æ®µå¤„ç†å‚æ•° - 40ç§’ç‰ˆæœ¬
SEGMENT_DURATION_MS = 40000  # æ•°æ®æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰- 40ç§’
PEAK_DETECTION_PARAMS = {
    'distance': 5,           # å³°å€¼é—´æœ€å°è·ç¦»
    'prominence': 25,        # å³°å€¼çªå‡ºåº¦
    'height': None           # å³°å€¼é«˜åº¦é˜ˆå€¼ï¼ˆNoneä¸ºè‡ªåŠ¨ï¼‰
}

# è´¨é‡è¯„ä¼°å‚æ•° - é’ˆå¯¹40ç§’æ•°æ®æ®µè°ƒæ•´
QUALITY_PARAMS = {
    'min_peaks_per_segment': 25,     # æ¯æ®µæœ€å°‘å³°å€¼æ•°ï¼ˆ40ç§’éœ€è¦æ›´å¤šå³°å€¼ï¼‰
    'max_peaks_per_segment': 80,     # æ¯æ®µæœ€å¤šå³°å€¼æ•°ï¼ˆ40ç§’å¯ä»¥å®¹çº³æ›´å¤šå³°å€¼ï¼‰
    'gap_threshold_factor': 2.5,     # æ–­ç‚¹æ£€æµ‹é˜ˆå€¼å› å­ï¼ˆå€æ•°ï¼‰
    'rr_variability_threshold': 0.3, # RRé—´æœŸå˜å¼‚æ€§é˜ˆå€¼
    'outlier_threshold': 2.5,        # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼ï¼ˆZ-scoreï¼‰
    'rr_range_min_factor': 0.7,      # RRé—´éš”èŒƒå›´ä¸‹é™å› å­ï¼ˆ70%ï¼‰
    'rr_range_max_factor': 1.3,      # RRé—´éš”èŒƒå›´ä¸Šé™å› å­ï¼ˆ130%ï¼‰
    'min_segment_quality_score': 1.1 # æœ€ä½è´¨é‡è¯„åˆ†
}

# --------------------------------------------------------------------------------------------
# æ™ºèƒ½æ•°æ®æ®µå¤„ç†æ ¸å¿ƒå‡½æ•°
# --------------------------------------------------------------------------------------------

def detect_all_peaks(data: pd.DataFrame) -> tuple:
    """å¯¹æ•´æ®µæ•°æ®è¿›è¡Œå³°å€¼æ£€æµ‹"""
    signal_values = data['æ•°å€¼'].values
    
    # æ‰§è¡Œå³°å€¼æ£€æµ‹
    peaks, properties = find_peaks(
        signal_values,
        distance=PEAK_DETECTION_PARAMS['distance'],
        prominence=PEAK_DETECTION_PARAMS['prominence'],
        height=PEAK_DETECTION_PARAMS['height']
    )
    
    if len(peaks) == 0:
        return None, None
    
    # è®¡ç®—å³°å€¼æ—¶é—´å’ŒRRé—´æœŸ
    peak_times = data['æ—¶é—´'].iloc[peaks].values
    rr_intervals = np.diff(peak_times)
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    peak_info = {
        'indices': peaks,
        'times': peak_times,
        'values': signal_values[peaks],
        'rr_intervals': rr_intervals,
        'mean_rr': np.mean(rr_intervals),
        'std_rr': np.std(rr_intervals)
    }
    
    return peak_info, properties

def detect_gaps_and_anomalies(peak_info: dict) -> np.ndarray:
    """æ£€æµ‹å³°å€¼é—´çš„æ–­ç‚¹å’Œå¼‚å¸¸"""
    if peak_info is None or len(peak_info['rr_intervals']) == 0:
        return np.array([])
    
    rr_intervals = peak_info['rr_intervals']
    mean_rr = peak_info['mean_rr']
    
    # æ–¹æ³•1: åŸºäºé˜ˆå€¼å› å­çš„æ–­ç‚¹æ£€æµ‹
    gap_threshold = mean_rr * QUALITY_PARAMS['gap_threshold_factor']
    gap_indices = np.where(rr_intervals > gap_threshold)[0]
    
    # æ–¹æ³•2: åŸºäºZ-scoreçš„å¼‚å¸¸æ£€æµ‹
    outlier_indices = np.array([])
    try:
        if len(rr_intervals) > 1 and np.std(rr_intervals) > 1e-10:
            z_scores = np.abs(zscore(rr_intervals))
            valid_z_mask = np.isfinite(z_scores)
            if np.any(valid_z_mask):
                outlier_indices = np.where((z_scores > QUALITY_PARAMS['outlier_threshold']) & valid_z_mask)[0]
    except Exception:
        outlier_indices = np.array([])
    
    # åˆå¹¶å¼‚å¸¸ä½ç½®
    if len(gap_indices) > 0 and len(outlier_indices) > 0:
        anomaly_indices = np.unique(np.concatenate([gap_indices, outlier_indices]))
    elif len(gap_indices) > 0:
        anomaly_indices = gap_indices
    elif len(outlier_indices) > 0:
        anomaly_indices = outlier_indices
    else:
        anomaly_indices = np.array([])
    
    return anomaly_indices

def evaluate_segment_quality(data_segment: pd.DataFrame, segment_peak_times: np.ndarray, 
                           segment_start_time: float, segment_end_time: float) -> tuple:
    """è¯„ä¼°å•ä¸ª40sæ•°æ®æ®µçš„è´¨é‡ï¼Œè¿”å›(æ˜¯å¦é€šè¿‡, è´¨é‡ä¿¡æ¯)"""
    
    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å³°å€¼æ—¶é—´
    segment_peaks = segment_peak_times[(segment_peak_times >= segment_start_time) & (segment_peak_times <= segment_end_time)]
    
    quality_score = 0.0
    issues = []
    
    # æ£€æŸ¥1: å³°å€¼æ•°é‡ - ç›´æ¥å‰”é™¤ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®æ®µ
    peak_count = len(segment_peaks)
    if peak_count < QUALITY_PARAMS['min_peaks_per_segment']:
        return False, {
            'quality_score': 0.0,
            'peak_count': peak_count,
            'issues': [f"å³°å€¼æ•°é‡ä¸è¶³({peak_count}<{QUALITY_PARAMS['min_peaks_per_segment']})"],
            'rr_mean': 0,
            'rr_std': 0,
            'completeness': 0
        }
    elif peak_count > QUALITY_PARAMS['max_peaks_per_segment']:
        return False, {
            'quality_score': 0.0,
            'peak_count': peak_count,
            'issues': [f"å³°å€¼æ•°é‡è¿‡å¤š({peak_count}>{QUALITY_PARAMS['max_peaks_per_segment']})"],
            'rr_mean': 0,
            'rr_std': 0,
            'completeness': 0
        }
    else:
        quality_score += 0.3  # å³°å€¼æ•°é‡åˆç†
    
    # æ£€æŸ¥2: RRé—´æœŸå˜å¼‚æ€§ - ç›´æ¥å‰”é™¤å˜å¼‚æ€§è¿‡é«˜çš„æ•°æ®æ®µ
    if len(segment_peaks) > 1:
        rr_intervals = np.diff(segment_peaks)
        mean_rr = np.mean(rr_intervals)
        std_rr = np.std(rr_intervals)
        cv_rr = std_rr / mean_rr if mean_rr > 0 else float('inf')
        
        if cv_rr > QUALITY_PARAMS['rr_variability_threshold']:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"RRé—´æœŸå˜å¼‚æ€§è¿‡é«˜(CV={cv_rr:.3f})"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.3  # å˜å¼‚æ€§åˆç†
        
        # æ£€æŸ¥3: æ–­ç‚¹æ£€æµ‹ - ç›´æ¥å‰”é™¤å­˜åœ¨æ–­ç‚¹çš„æ•°æ®æ®µ
        gap_threshold = mean_rr * QUALITY_PARAMS['gap_threshold_factor']
        gaps = np.sum(rr_intervals > gap_threshold)
        if gaps > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{gaps}ä¸ªæ–­ç‚¹"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ— æ–­ç‚¹
        
        # æ£€æŸ¥4: å¼‚å¸¸å€¼æ£€æµ‹ - ç›´æ¥å‰”é™¤å­˜åœ¨å¼‚å¸¸å€¼çš„æ•°æ®æ®µ
        z_scores = np.abs(zscore(rr_intervals))
        outliers = np.sum(z_scores > QUALITY_PARAMS['outlier_threshold'])
        if outliers > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{outliers}ä¸ªå¼‚å¸¸å€¼"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ— å¼‚å¸¸å€¼
        
        # æ£€æŸ¥5: RRé—´éš”èŒƒå›´æ£€æŸ¥ï¼ˆ70%-130%ï¼‰- ç›´æ¥å‰”é™¤è¶…å‡ºèŒƒå›´çš„æ•°æ®æ®µ
        rr_min_threshold = mean_rr * QUALITY_PARAMS['rr_range_min_factor']
        rr_max_threshold = mean_rr * QUALITY_PARAMS['rr_range_max_factor']
        out_of_range_count = np.sum((rr_intervals < rr_min_threshold) | (rr_intervals > rr_max_threshold))
        if out_of_range_count > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{out_of_range_count}ä¸ªRRé—´éš”è¶…å‡º70%-130%èŒƒå›´"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ‰€æœ‰RRé—´éš”åœ¨åˆç†èŒƒå›´å†…
    
    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    expected_points = int(SEGMENT_DURATION_MS * len(data_segment) / (data_segment['æ—¶é—´'].max() - data_segment['æ—¶é—´'].min()))
    actual_points = len(data_segment)
    completeness = min(1.0, actual_points / expected_points)
    quality_score += 0.2 * completeness
    
    # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ï¼Œè¿”å›é€šè¿‡çŠ¶æ€
    return True, {
        'quality_score': quality_score,
        'peak_count': peak_count,
        'issues': [],
        'rr_mean': np.mean(rr_intervals) if len(segment_peaks) > 1 else 0,
        'rr_std': np.std(rr_intervals) if len(segment_peaks) > 1 else 0,
        'completeness': completeness
    }

def generate_candidate_segments(data: pd.DataFrame, peak_info: dict) -> list:
    """ç”Ÿæˆå€™é€‰çš„40sæ•°æ®æ®µ - åŸºäºå³°å€¼ä½ç½®çš„æ»‘åŠ¨çª—å£"""
    
    if peak_info is None or len(peak_info['times']) < 2:
        return []
    
    # æ£€æµ‹å¼‚å¸¸ä½ç½®
    anomaly_indices = detect_gaps_and_anomalies(peak_info)
    
    # è·å–æ‰€æœ‰å³°å€¼æ—¶é—´
    peak_times = peak_info['times']
    data_start = data['æ—¶é—´'].min()
    data_end = data['æ—¶é—´'].max()
    
    candidates = []
    
    # ä»æ¯ä¸ªå³°å€¼å¼€å§‹ï¼Œå°è¯•ç”Ÿæˆ40sçš„æ•°æ®æ®µ
    for i, start_peak_time in enumerate(peak_times):
        # è®¡ç®—çª—å£ç»“æŸæ—¶é—´
        end_time = start_peak_time + SEGMENT_DURATION_MS
        
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ•°æ®èŒƒå›´
        if end_time > data_end:
            break
        
        # æå–æ•°æ®æ®µ
        mask = (data['æ—¶é—´'] >= start_peak_time) & (data['æ—¶é—´'] <= end_time)
        segment_data = data[mask].copy().reset_index(drop=True)
        
        if len(segment_data) < 200:  # 40ç§’éœ€è¦æ›´å¤šæ•°æ®ç‚¹
            continue
        
        # æ‰¾åˆ°è¯¥æ®µå†…çš„å³°å€¼æ—¶é—´
        segment_peak_times = peak_times[(peak_times >= start_peak_time) & (peak_times <= end_time)]
        
        # è¯„ä¼°è´¨é‡ - ç›´æ¥å‰”é™¤æœ‰é—®é¢˜çš„æ•°æ®æ®µ
        quality_passed, quality = evaluate_segment_quality(segment_data, segment_peak_times, start_peak_time, end_time)
        
        # åªä¿ç•™é€šè¿‡è´¨é‡æ£€æŸ¥çš„æ®µ
        if quality_passed:
            candidates.append({
                'start_time': start_peak_time,
                'end_time': end_time,
                'start_peak_index': i,  # è®°å½•èµ·å§‹å³°å€¼ç´¢å¼•
                'data': segment_data,
                'peak_times': segment_peak_times,
                'quality': quality
            })
    
    # æŒ‰è´¨é‡è¯„åˆ†æ’åº
    candidates.sort(key=lambda x: x['quality']['quality_score'], reverse=True)
    
    # å»é‡ï¼šå¦‚æœä¸¤ä¸ªå€™é€‰æ®µé‡å åº¦å¾ˆé«˜ï¼Œåªä¿ç•™è´¨é‡æ›´é«˜çš„
    filtered_candidates = []
    for candidate in candidates:
        is_duplicate = False
        for existing in filtered_candidates:
            # è®¡ç®—é‡å åº¦
            overlap_start = max(candidate['start_time'], existing['start_time'])
            overlap_end = min(candidate['end_time'], existing['end_time'])
            overlap_duration = max(0, overlap_end - overlap_start)
            overlap_ratio = overlap_duration / SEGMENT_DURATION_MS
            
            # å¦‚æœé‡å è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
            if overlap_ratio > 0.7:
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered_candidates.append(candidate)
    
    return filtered_candidates

# --------------------------------------------------------------------------------------------
# HRVç‰¹å¾è®¡ç®—æ ¸å¿ƒå‡½æ•°
# --------------------------------------------------------------------------------------------

def read_rr_data(path: str) -> np.ndarray:
    """è¯»å–RRé—´æœŸæ•°æ®ï¼ˆæ¯«ç§’ï¼‰"""
    data = pd.read_csv(path, header=None)
    if data.shape[1] < 1:
        raise ValueError("RRæ–‡ä»¶éœ€è¦è‡³å°‘ä¸€åˆ—æ•°æ®")
    rr_ms = data.iloc[:, 0].to_numpy(dtype=float)
    
    return rr_ms

def calculate_hrv_features(rr_ms: np.ndarray) -> dict:
    """è®¡ç®—æŒ‡å®š7é¡¹HRVç‰¹å¾ã€‚è¾“å…¥å•ä½: ms - ä¼˜åŒ–ç‰ˆæœ¬"""
    if rr_ms is None or len(rr_ms) < 2:
        return {}

    feat = {}

    # --- æ—¶åŸŸç‰¹å¾ - å‘é‡åŒ–è®¡ç®— ---
    rr_diff = np.diff(rr_ms)
    
    # RMSSD - å‘é‡åŒ–è®¡ç®—
    feat['RMSSD'] = float(np.sqrt(np.mean(rr_diff ** 2))) if len(rr_diff) > 0 else np.nan
    
    # pNN58 - å‘é‡åŒ–è®¡ç®—
    if len(rr_diff) > 0:
        over_58_count = np.sum(np.abs(rr_diff) > 58.0)
        feat['pNN58'] = float((over_58_count / len(rr_diff)) * 100.0)
    else:
        feat['pNN58'] = np.nan
    
    # SDNN - å‘é‡åŒ–è®¡ç®—
    feat['SDNN'] = float(np.std(rr_ms, ddof=1)) if len(rr_ms) > 1 else np.nan

    # --- PoincarÃ©ç‰¹å¾ - å‘é‡åŒ–è®¡ç®— ---
    if len(rr_diff) > 0:
        var_diff = np.var(rr_diff)
        var_rr = np.var(rr_ms, ddof=1)
        
        sd1 = float(np.sqrt(var_diff / 2.0))
        sd2 = float(np.sqrt(2 * var_rr - var_diff / 2.0))
        
        feat['SD1'] = sd1
        feat['SD2'] = sd2
        feat['SD1_SD2'] = float(sd1 / sd2) if sd2 > 0 else np.nan
    else:
        feat['SD1'] = np.nan
        feat['SD2'] = np.nan
        feat['SD1_SD2'] = np.nan

    # --- SampEn --- é«˜æ•ˆç‰ˆæœ¬ï¼Œä½¿ç”¨è¿‘ä¼¼ç®—æ³•
    sampen = np.nan
    try:
        if len(rr_ms) >= 10:
            # ä½¿ç”¨æ›´å¿«çš„è¿‘ä¼¼ç®—æ³•
            m = 2
            r = 0.2 * np.std(rr_ms)
            
            # é™åˆ¶æ•°æ®é•¿åº¦ä»¥æé«˜æ€§èƒ½
            max_len = min(len(rr_ms), 500)  # è¿›ä¸€æ­¥é™åˆ¶é•¿åº¦
            rr_subset = rr_ms[:max_len]
            
            # ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•
            count_m = 0
            count_m1 = 0
            total_m = 0
            total_m1 = 0
            
            # é¢„è®¡ç®—æ‰€æœ‰çª—å£
            windows_m = np.array([rr_subset[i:i+m] for i in range(len(rr_subset) - m)])
            windows_m1 = np.array([rr_subset[i:i+m+1] for i in range(len(rr_subset) - m - 1)])
            
            # å‘é‡åŒ–æ¯”è¾ƒ
            for i in range(len(windows_m)):
                # é™åˆ¶æ¯”è¾ƒèŒƒå›´ä»¥æé«˜æ€§èƒ½
                end_idx = min(i + 30, len(windows_m))
                
                for j in range(i + 1, end_idx):
                    if np.max(np.abs(windows_m[i] - windows_m[j])) <= r:
                        count_m += 1
                    total_m += 1
                    
                    if i < len(windows_m1) and j < len(windows_m1):
                        if np.max(np.abs(windows_m1[i] - windows_m1[j])) <= r:
                            count_m1 += 1
                        total_m1 += 1
            
            if count_m > 0 and count_m1 > 0 and total_m > 0 and total_m1 > 0:
                sampen = -np.log((count_m1 / total_m1) / (count_m / total_m))
    except Exception:
        pass
    feat['SampEn'] = sampen

    return feat

# --------------------------------------------------------------------------------------------
# å¤šçº¿ç¨‹å¤„ç†ç±»
# --------------------------------------------------------------------------------------------

class MultiThreadHRVProcessor:
    def __init__(self, input_dir: str, output_file: str, max_workers: int = 8):
        self.input_dir = input_dir
        self.output_file = output_file
        self.max_workers = max_workers
        self.lock = threading.Lock()  # ç”¨äºä¿æŠ¤æ–‡ä»¶å†™å…¥æ“ä½œ
        self.results = []  # å­˜å‚¨æ‰€æœ‰å¤„ç†ç»“æœ
        self.processing_status = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†çŠ¶æ€
        self.thread_status = {}  # å­˜å‚¨æ¯ä¸ªçº¿ç¨‹çš„çŠ¶æ€
        self.monitor_running = False  # ç›‘æ§çº¿ç¨‹è¿è¡ŒçŠ¶æ€
        self.performance_stats = {
            'start_time': 0,
            'processed_files': 0,
            'total_files': 0,
            'cpu_usage': 0,
            'memory_usage': 0
        }
        self.rr_backup_enabled = bool(RR_BACKUP_BASE_DIR.strip())
        self.rr_backup_dirs = {}  # å­˜å‚¨æ¯ä¸ªæ ‡ç­¾çš„å¤‡ä»½ç›®å½•
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file) if os.path.dirname(output_file) else "."
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
    
    def process_single_csv(self, csv_file: str, emotion_label: str, thread_id: int) -> pd.DataFrame:
        """å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼Œå…ˆè¿›è¡Œæ™ºèƒ½æ•°æ®æ®µå¤„ç†ï¼Œç„¶åè®¡ç®—HRVç‰¹å¾"""
        filename = os.path.basename(csv_file)
        
        try:
            # æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼Œå‡å°‘é”æ“ä½œ
            self._update_status_batch(thread_id, filename, "è¯»å–ä¸­")
            
            # è¯»å–æ•°æ®
            data = pd.read_csv(csv_file, header=None)
            
            if data.shape[1] < 2:
                self._update_status_batch(thread_id, filename, "æ ¼å¼é”™è¯¯")
                print(f"\nâŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {filename} (éœ€è¦è‡³å°‘2åˆ—æ•°æ®)")
                return pd.DataFrame()
            
            # è®¾ç½®åˆ—å
            data = data.iloc[:, :2].copy()
            data.columns = ['æ—¶é—´', 'æ•°å€¼']
            
            if len(data) < 200:  # 40ç§’éœ€è¦æ›´å¤šæ•°æ®ç‚¹
                self._update_status_batch(thread_id, filename, "æ•°æ®ä¸è¶³")
                print(f"\nâŒ æ•°æ®ä¸è¶³: {filename} (éœ€è¦è‡³å°‘200ä¸ªæ•°æ®ç‚¹)")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "å³°å€¼æ£€æµ‹ä¸­")
            
            # ç¬¬1æ­¥ï¼šå³°å€¼æ£€æµ‹
            peak_info, _ = detect_all_peaks(data)
            if peak_info is None:
                self._update_status_batch(thread_id, filename, "å³°å€¼æ£€æµ‹å¤±è´¥")
                print(f"\nâŒ å³°å€¼æ£€æµ‹å¤±è´¥: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "ç”Ÿæˆæ•°æ®æ®µä¸­")
            
            # ç¬¬2æ­¥ï¼šç”Ÿæˆå€™é€‰æ•°æ®æ®µ
            candidates = generate_candidate_segments(data, peak_info)
            if not candidates:
                self._update_status_batch(thread_id, filename, "æ— é«˜è´¨é‡æ•°æ®æ®µ")
                print(f"\nâŒ æ— é«˜è´¨é‡æ•°æ®æ®µ: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "è®¡ç®—HRVç‰¹å¾ä¸­")
            
            # ç¬¬3æ­¥ï¼šå¯¹æ¯ä¸ªé«˜è´¨é‡æ•°æ®æ®µè®¡ç®—HRVç‰¹å¾
            results = []
            for i, segment in enumerate(candidates):
                # ä»æ•°æ®æ®µä¸­æå–RRé—´æœŸ
                if len(segment['peak_times']) > 1:
                    rr_intervals = np.diff(segment['peak_times'])
                    
                    # å¤‡ä»½RRé—´éš”æ•°æ®
                    backup_path = self._backup_rr_data(rr_intervals, filename, emotion_label, i+1)
                    
                    # è®¡ç®—HRVç‰¹å¾
                    feat = calculate_hrv_features(rr_intervals)
                    
                    if feat:
                        # æ„å»ºç»“æœè¡Œ
                        row = {
                            'file': f"{filename}_seg{i+1}",
                            'RMSSD': feat.get('RMSSD', np.nan),
                            'pNN58': feat.get('pNN58', np.nan),
                            'SDNN': feat.get('SDNN', np.nan),
                            'SD1': feat.get('SD1', np.nan),
                            'SD2': feat.get('SD2', np.nan),
                            'SD1_SD2': feat.get('SD1_SD2', np.nan),
                            'SampEn': feat.get('SampEn', np.nan),
                            'emotion': emotion_label
                        }
                        results.append(row)
            
            if not results:
                self._update_status_batch(thread_id, filename, "HRVè®¡ç®—å¤±è´¥")
                print(f"\nâŒ HRVç‰¹å¾è®¡ç®—å¤±è´¥: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, f"å·²å®Œæˆ({len(results)}æ®µ)")
            
            return pd.DataFrame(results)
            
        except Exception as e:
            self._update_status_batch(thread_id, filename, "å¤„ç†å¤±è´¥")
            print(f"\nâŒ å¤„ç†å¤±è´¥: {filename} -> {e}")
            return pd.DataFrame()
    
    def _update_status_batch(self, thread_id: int, filename: str, status: str):
        """æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼Œå‡å°‘é”æ“ä½œé¢‘ç‡"""
        with self.lock:
            self.thread_status[thread_id] = f"{status} {filename}"
            self.processing_status[filename] = status
    
    def _get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_percent = psutil.virtual_memory().percent
            return cpu_percent, memory_percent
        except:
            return 0, 0
    
    def _update_performance_stats(self, processed_count: int):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            self.performance_stats['processed_files'] = processed_count
            cpu_usage, memory_usage = self._get_performance_stats()
            self.performance_stats['cpu_usage'] = cpu_usage
            self.performance_stats['memory_usage'] = memory_usage
    
    def _setup_rr_backup_dir(self, emotion_label: str) -> str:
        """ä¸ºæ¯ä¸ªæ ‡ç­¾è®¾ç½®RRæ•°æ®å¤‡ä»½ç›®å½•"""
        if not self.rr_backup_enabled:
            return None
        
        # æ¸…ç†æ ‡ç­¾åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_label = emotion_label.replace(" ", "_").replace("/", "_").replace("\\", "_")
        backup_dir = os.path.join(RR_BACKUP_BASE_DIR, clean_label)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆä½¿ç”¨exist_ok=Trueé¿å…ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯ï¼‰
        try:
            os.makedirs(backup_dir, exist_ok=True)
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {backup_dir}, é”™è¯¯: {e}")
            return None
        
        return backup_dir
    
    def _backup_rr_data(self, rr_intervals: np.ndarray, filename: str, emotion_label: str, segment_index: int) -> str:
        """å¤‡ä»½RRé—´éš”æ•°æ®åˆ°æŒ‡å®šç›®å½•"""
        if not self.rr_backup_enabled:
            return None
        
        # è·å–æˆ–åˆ›å»ºå¤‡ä»½ç›®å½•
        if emotion_label not in self.rr_backup_dirs:
            self.rr_backup_dirs[emotion_label] = self._setup_rr_backup_dir(emotion_label)
        
        backup_dir = self.rr_backup_dirs[emotion_label]
        if not backup_dir:
            return None
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        base_name = os.path.splitext(filename)[0]
        backup_filename = f"{base_name}_seg{segment_index}_RR_40s.csv"
        backup_path = os.path.join(backup_dir, backup_filename)
        
        try:
            # ä¿å­˜RRé—´éš”æ•°æ®
            pd.DataFrame(rr_intervals).to_csv(backup_path, index=False, header=False)
            return backup_path
        except Exception as e:
            print(f"\nâš ï¸ RRæ•°æ®å¤‡ä»½å¤±è´¥: {backup_path} - {e}")
            return None
    
    def process_all_files(self, csv_files: List[str], emotion_labels: List[str]) -> None:
        """å¤šçº¿ç¨‹å¤„ç†æ‰€æœ‰CSVæ–‡ä»¶"""
        if len(csv_files) != len(emotion_labels):
            print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶æ•°é‡({len(csv_files)})ä¸æ ‡ç­¾æ•°é‡({len(emotion_labels)})ä¸åŒ¹é…")
            return
        
        print(f"ğŸ“Š å¼€å§‹å¤šçº¿ç¨‹å¤„ç† {len(csv_files)} ä¸ªæ–‡ä»¶ (40ç§’æ•°æ®æ®µ)")
        print(f"ğŸ”§ ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        print()
        
        # é»˜è®¤å¯ç”¨å®æ—¶ç›‘æ§
        monitor_thread = self._start_status_monitor()
        print("âœ… å®æ—¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C å¯åœæ­¢ç›‘æ§")
        time.sleep(1)
        
        start_time = time.time()
        success_count = 0
        
        # åˆå§‹åŒ–æ€§èƒ½ç»Ÿè®¡
        self.performance_stats['start_time'] = start_time
        self.performance_stats['total_files'] = len(csv_files)
        
        # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
        for csv_file in csv_files:
            self.processing_status[os.path.basename(csv_file)] = "ç­‰å¾…ä¸­"
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…çº¿ç¨‹ID
            future_to_file = {}
            for i, (csv_file, emotion_label) in enumerate(zip(csv_files, emotion_labels)):
                future = executor.submit(self.process_single_csv, csv_file, emotion_label, i % self.max_workers)
                future_to_file[future] = (csv_file, emotion_label)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            completed_count = 0
            for future in as_completed(future_to_file):
                csv_file, emotion_label = future_to_file[future]
                filename = os.path.basename(csv_file)
                
                try:
                    result = future.result()
                    if not result.empty:
                        with self.lock:
                            self.results.append(result)
                        success_count += 1
                        self.processing_status[filename] = "âœ… æˆåŠŸ"
                    else:
                        self.processing_status[filename] = "âŒ å¤±è´¥"
                except Exception as e:
                    print(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {filename} - {e}")
                    self.processing_status[filename] = "âŒ å¼‚å¸¸"
                
                completed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if not self.monitor_running:  # å¦‚æœæ²¡æœ‰å¯ç”¨å®æ—¶ç›‘æ§ï¼Œæ˜¾ç¤ºç®€å•è¿›åº¦
                    progress = (completed_count / len(csv_files)) * 100
                    # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                    self._update_performance_stats(completed_count)
                    
                    # è®¡ç®—å¤„ç†é€Ÿåº¦
                    elapsed_time = time.time() - start_time
                    files_per_second = completed_count / elapsed_time if elapsed_time > 0 else 0
                    
                    print(f"\rğŸ”„ å¤„ç†è¿›åº¦: {completed_count}/{len(csv_files)} ({progress:.1f}%) - æˆåŠŸ: {success_count}, å¤±è´¥: {completed_count - success_count} - é€Ÿåº¦: {files_per_second:.1f} æ–‡ä»¶/ç§’ - CPU: {self.performance_stats['cpu_usage']:.1f}%", end="", flush=True)
        
        end_time = time.time()
        
        # åœæ­¢ç›‘æ§
        if monitor_thread:
            self._stop_status_monitor()
            time.sleep(1)  # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"æ€»æ–‡ä»¶æ•°: {len(csv_files)}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤±è´¥æ–‡ä»¶: {len(csv_files) - success_count}")
        print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’")
        
        # æ˜¾ç¤ºè¯¦ç»†å¤„ç†ç»“æœ
        self._show_detailed_results()
    
    def _show_detailed_results(self):
        """æ˜¾ç¤ºè¯¦ç»†å¤„ç†ç»“æœ"""
        print(f"\nğŸ“‹ è¯¦ç»†å¤„ç†ç»“æœ:")
        print("-" * 80)
        
        # æŒ‰çŠ¶æ€åˆ†ç»„æ˜¾ç¤º
        status_groups = {}
        for filename, status in self.processing_status.items():
            if status not in status_groups:
                status_groups[status] = []
            status_groups[status].append(filename)
        
        for status, files in status_groups.items():
            print(f"{status}: {len(files)} ä¸ªæ–‡ä»¶")
            for filename in sorted(files):
                print(f"  - {filename}")
            print()
    
    def _start_status_monitor(self):
        """å¯åŠ¨çŠ¶æ€ç›‘æ§çº¿ç¨‹"""
        def monitor():
            while self.monitor_running:
                with self.lock:
                    # æ¸…å±å¹¶æ˜¾ç¤ºå½“å‰çŠ¶æ€
                    os.system('cls' if os.name == 'nt' else 'clear')
                    print("ğŸ”„ å®æ—¶å¤„ç†çŠ¶æ€ç›‘æ§ (40ç§’æ•°æ®æ®µ - æ‰¹é‡ç‰ˆæœ¬)")
                    print("=" * 60)
                    
                    # æ˜¾ç¤ºçº¿ç¨‹çŠ¶æ€
                    print("ğŸ§µ çº¿ç¨‹çŠ¶æ€:")
                    for thread_id in range(self.max_workers):
                        status = self.thread_status.get(thread_id, "ç©ºé—²")
                        print(f"  çº¿ç¨‹ {thread_id}: {status}")
                    
                    print("\nğŸ“ æ–‡ä»¶å¤„ç†çŠ¶æ€:")
                    # æŒ‰çŠ¶æ€åˆ†ç»„æ˜¾ç¤º
                    status_groups = {}
                    for filename, status in self.processing_status.items():
                        if status not in status_groups:
                            status_groups[status] = []
                        status_groups[status].append(filename)
                    
                    for status, files in status_groups.items():
                        print(f"  {status}: {len(files)} ä¸ªæ–‡ä»¶")
                        for filename in sorted(files)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            print(f"    - {filename}")
                        if len(files) > 5:
                            print(f"    ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
                    
                    print(f"\nâ±ï¸  æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
                
                time.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
        
        self.monitor_running = True
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        return monitor_thread
    
    def _stop_status_monitor(self):
        """åœæ­¢çŠ¶æ€ç›‘æ§"""
        self.monitor_running = False
    
    def save_results(self) -> None:
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        df = pd.concat(self.results, ignore_index=True)
        
        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
        cols = ['file', 'RMSSD', 'pNN58', 'SDNN', 'SD1', 'SD2', 'SD1_SD2', 'SampEn', 'emotion']
        for c in cols:
            if c not in df.columns:
                df[c] = np.nan
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        df = df[cols]
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯æ–°å»º
        file_exists = os.path.exists(self.output_file)
        
        if file_exists:
            # è¿½åŠ æ¨¡å¼ï¼šä¸å†™è¡¨å¤´ï¼Œåªè¿½åŠ æ•°æ®è¡Œ
            df.to_csv(self.output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ æ–°è®°å½•å·²è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶: {self.output_file}")
        else:
            # æ–°å»ºæ¨¡å¼ï¼šå†™è¡¨å¤´å’Œæ•°æ®
            df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ HRVç‰¹å¾æ•°æ®é›†å·²åˆ›å»º: {self.output_file}")
        
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {df.shape}")
        print(f"ğŸ”¢ ç‰¹å¾æ•°é‡: {len(df.columns) - 2}")  # å‡å»fileå’Œemotionåˆ—
        print(f"ğŸ“ˆ æ ·æœ¬æ•°é‡: {len(df)}")
        
        # æ˜¾ç¤ºç‰¹å¾åˆ—ä¿¡æ¯
        print(f"ğŸ·ï¸ ç‰¹å¾åˆ—: {list(df.columns[1:-1])}")  # é™¤äº†fileå’Œemotion
        print(f"ğŸ·ï¸ æ ‡ç­¾åˆ—: {df.columns[-1]}")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
        print(f"\nğŸ“‹ æ•°æ®é›†å‰3è¡Œ:")
        print(df.head(3))
        
        # æ˜¾ç¤ºRRæ•°æ®å¤‡ä»½ä¿¡æ¯
        if self.rr_backup_enabled and self.rr_backup_dirs:
            print(f"\nğŸ’¾ RRé—´éš”æ•°æ®å¤‡ä»½ä¿¡æ¯ (40ç§’ç‰ˆæœ¬ - æ‰¹é‡):")
            for emotion_label, backup_dir in self.rr_backup_dirs.items():
                if backup_dir and os.path.exists(backup_dir):
                    backup_files = glob.glob(os.path.join(backup_dir, "*_RR_40s.csv"))
                    print(f"   {emotion_label}: {len(backup_files)} ä¸ªæ–‡ä»¶ -> {backup_dir}")
        elif self.rr_backup_enabled:
            print(f"\nâš ï¸ RRæ•°æ®å¤‡ä»½åŠŸèƒ½å·²å¯ç”¨ï¼Œä½†æœªè®¾ç½®å¤‡ä»½ç›®å½•è·¯å¾„")
        else:
            print(f"\nğŸ’¾ RRæ•°æ®å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")

# --------------------------------------------------------------------------------------------
# æ‰¹é‡è¾“å…¥å¤„ç†å‡½æ•°
# --------------------------------------------------------------------------------------------

def parse_batch_input() -> Dict[str, List[Tuple[str, str]]]:
    """è§£ææ‰¹é‡è¾“å…¥æ ¼å¼ï¼Œè¿”å› {äººå: [(æƒ…ç»ªæ ‡ç­¾, åˆ†æ•°), ...]} çš„å­—å…¸"""
    print("=== æ‰¹é‡è¾“å…¥æ ¼å¼è¯´æ˜ ===")
    print("è¯·è¾“å…¥æ‰¹é‡æ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š")
    print("1äººå")
    print("æƒ…ç»ªæ ‡ç­¾1 å¯¹åº”åˆ†æ•°")
    print("æƒ…ç»ªæ ‡ç­¾2 å¯¹åº”åˆ†æ•°")
    print("2äººå")
    print("æƒ…ç»ªæ ‡ç­¾1 å¯¹åº”åˆ†æ•°")
    print("...")
    print("è¾“å…¥å®ŒæˆåæŒ‰Ctrl+Zç„¶åå›è½¦ç»“æŸ")
    print()
    print("ç¤ºä¾‹ï¼š")
    print("1è°­æ€¡é›…")
    print("å¹³é™")
    print("æ„‰æ‚¦ 6")
    print("æ„‰æ‚¦ 7")
    print("æ„‰æ‚¦ 8")
    print("2å”é“­é¥")
    print("ç´§å¼  9")
    print("æ„‰æ‚¦ 7")
    print("å¹³é™ 5")
    print()
    
    batch_data = {}
    current_person = None
    
    try:
        while True:
            line = input().strip()
            if not line:  # å¿½ç•¥ç©ºè¡Œ
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯äººå‘˜æ ‡è®°è¡Œï¼ˆä»¥æ•°å­—å¼€å¤´ï¼‰
            if line[0].isdigit():
                # æå–äººåï¼ˆå»æ‰å¼€å¤´çš„æ•°å­—ï¼‰
                person_name = line[1:].strip()
                if person_name:
                    current_person = person_name
                    batch_data[current_person] = []
                    print(f"âœ… æ·»åŠ äººå‘˜: {person_name}")
            else:
                # æƒ…ç»ªæ ‡ç­¾è¡Œ
                if current_person is None:
                    print(f"âš ï¸ è­¦å‘Š: åœ¨æœªæŒ‡å®šäººå‘˜çš„æƒ…å†µä¸‹è¾“å…¥æƒ…ç»ªæ ‡ç­¾: {line}")
                    continue
                
                # è§£ææƒ…ç»ªæ ‡ç­¾å’Œåˆ†æ•°
                parts = line.split()
                if len(parts) == 1:
                    # åªæœ‰æƒ…ç»ªæ ‡ç­¾ï¼Œæ²¡æœ‰åˆ†æ•°
                    emotion = parts[0]
                    score = ""
                elif len(parts) == 2:
                    # æœ‰æƒ…ç»ªæ ‡ç­¾å’Œåˆ†æ•°
                    emotion, score = parts
                else:
                    # å¤šä¸ªéƒ¨åˆ†ï¼Œå–ç¬¬ä¸€ä¸ªä½œä¸ºæƒ…ç»ªï¼Œç¬¬äºŒä¸ªä½œä¸ºåˆ†æ•°
                    emotion = parts[0]
                    score = parts[1] if len(parts) > 1 else ""
                
                # æ„å»ºæ ‡ç­¾ï¼ˆåŒ…å«åˆ†æ•°ä¿¡æ¯ï¼‰
                if score:
                    emotion_label = f"{emotion} {score}"
                else:
                    emotion_label = emotion
                
                batch_data[current_person].append((emotion, emotion_label))
                print(f"  âœ… æ·»åŠ æ ‡ç­¾: {emotion_label}")
                
    except EOFError:
        pass
    
    return batch_data

def assign_files_to_persons(batch_data: Dict[str, List[Tuple[str, str]]], all_csv_files: List[str]) -> List[Tuple[str, str]]:
    """æŒ‰é¡ºåºå°†CSVæ–‡ä»¶åˆ†é…ç»™å„ä¸ªäººå‘˜"""
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    sorted_csv_files = sorted(all_csv_files, key=lambda x: os.path.basename(x))
    
    all_file_emotion_pairs = []
    file_index = 0
    
    # æŒ‰äººå‘˜é¡ºåºå¤„ç†
    for person_name, emotions in batch_data.items():
        print(f"ğŸ” ä¸º {person_name} åˆ†é…æ–‡ä»¶...")
        
        # ä¸ºè¯¥äººå‘˜åˆ†é…å¯¹åº”æ•°é‡çš„æ–‡ä»¶
        person_files = []
        for i in range(len(emotions)):
            if file_index < len(sorted_csv_files):
                person_files.append(sorted_csv_files[file_index])
                file_index += 1
            else:
                print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œ{person_name} åªèƒ½åˆ†é…åˆ° {len(person_files)} ä¸ªæ–‡ä»¶")
                break
        
        if not person_files:
            print(f"âŒ æœªä¸º {person_name} åˆ†é…åˆ°ä»»ä½•æ–‡ä»¶")
            continue
        
        print(f"âœ… ä¸º {person_name} åˆ†é…äº† {len(person_files)} ä¸ªæ–‡ä»¶:")
        for csv_file in person_files:
            print(f"   - {os.path.basename(csv_file)}")
        
        # æ„å»ºæ–‡ä»¶-æ ‡ç­¾å¯¹
        for i, (csv_file, (emotion, emotion_label)) in enumerate(zip(person_files, emotions)):
            # æ„å»ºå®Œæ•´çš„æ ‡ç­¾ï¼ˆäººå + æƒ…ç»ªæ ‡ç­¾ï¼‰
            full_emotion_label = f"{person_name} {emotion_label}"
            all_file_emotion_pairs.append((csv_file, full_emotion_label))
            print(f"   âœ… {os.path.basename(csv_file)} -> {full_emotion_label}")
        
        print()
    
    return all_file_emotion_pairs

# --------------------------------------------------------------------------------------------
# ä¸»å¤„ç†å‡½æ•°
# --------------------------------------------------------------------------------------------

def main():
    print("=== å¤šçº¿ç¨‹HRVç‰¹å¾è®¡ç®—è„šæœ¬ - æ‰¹é‡å¤„ç†ç‰ˆæœ¬ (40ç§’) ===")
    print(f"è¾“å…¥æ•°æ®ç›®å½•: {INPUT_DATA_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}")
    print(f"æ•°æ®æ®µé•¿åº¦: {SEGMENT_DURATION_MS/1000:.1f} ç§’")
    
    # æ˜¾ç¤ºRRå¤‡ä»½åŠŸèƒ½çŠ¶æ€
    if RR_BACKUP_BASE_DIR.strip():
        print(f"RRæ•°æ®å¤‡ä»½: å·²å¯ç”¨ -> {RR_BACKUP_BASE_DIR}")
    else:
        print(f"RRæ•°æ®å¤‡ä»½: æœªå¯ç”¨ (è¯·åœ¨è„šæœ¬ä¸­è®¾ç½® RR_BACKUP_BASE_DIR)")
    print()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(INPUT_DATA_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DATA_DIR}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    all_csv_files = sorted(glob.glob(os.path.join(INPUT_DATA_DIR, "*.csv")))
    if not all_csv_files:
        print(f"âŒ åœ¨ {INPUT_DATA_DIR} ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(all_csv_files)} ä¸ªCSVæ–‡ä»¶:")
    for i, csv_file in enumerate(all_csv_files, 1):
        print(f"   {i}. {os.path.basename(csv_file)}")
    print()
    
    # è§£ææ‰¹é‡è¾“å…¥
    batch_data = parse_batch_input()
    
    if not batch_data:
        print("âŒ æœªè¾“å…¥ä»»ä½•æ•°æ®")
        return
    
    print(f"\nğŸ“‹ è§£æç»“æœ:")
    for person_name, emotions in batch_data.items():
        print(f"ğŸ‘¤ {person_name}: {len(emotions)} ä¸ªæƒ…ç»ªæ ‡ç­¾")
        for emotion, emotion_label in emotions:
            print(f"   - {emotion_label}")
    print()
    
    # æŒ‰é¡ºåºåˆ†é…æ–‡ä»¶ç»™å„ä¸ªäººå‘˜
    all_file_emotion_pairs = assign_files_to_persons(batch_data, all_csv_files)
    
    if not all_file_emotion_pairs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶-æ ‡ç­¾å¯¹")
        return
    
    # åˆ†ç¦»æ–‡ä»¶å’Œæ ‡ç­¾
    csv_files = [pair[0] for pair in all_file_emotion_pairs]
    emotion_labels = [pair[1] for pair in all_file_emotion_pairs]
    
    print(f"ğŸ“Š æ€»è®¡å¤„ç† {len(csv_files)} ä¸ªæ–‡ä»¶-æ ‡ç­¾å¯¹")
    print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ:")
    label_counts = {}
    for label in emotion_labels:
        person = label.split()[0]
        if person not in label_counts:
            label_counts[person] = 0
        label_counts[person] += 1
    
    for person, count in label_counts.items():
        print(f"   {person}: {count} ä¸ªæ ‡ç­¾")
    print()
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶å¼€å§‹å¤„ç†
    processor = MultiThreadHRVProcessor(INPUT_DATA_DIR, OUTPUT_FILE, MAX_WORKERS)
    processor.process_all_files(csv_files, emotion_labels)
    
    # ä¿å­˜ç»“æœ
    processor.save_results()
    
    print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")

# --------------------------------------------------------------------------------------------
# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
# --------------------------------------------------------------------------------------------

def show_usage_example():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("=== ä½¿ç”¨ç¤ºä¾‹ - æ‰¹é‡å¤„ç†ç‰ˆæœ¬ (40ç§’) ===")
    print("1. ç¡®ä¿ processed_data ç›®å½•ä¸‹æœ‰å·²åˆ’åˆ†å¥½çš„CSVæ–‡ä»¶")
    print("2. è¿è¡Œè„šæœ¬åæŒ‰æç¤ºè¾“å…¥æ‰¹é‡æ•°æ®:")
    print("   1è°­æ€¡é›…")
    print("   å¹³é™")
    print("   æ„‰æ‚¦ 6")
    print("   æ„‰æ‚¦ 7")
    print("   æ„‰æ‚¦ 8")
    print("   2å”é“­é¥")
    print("   ç´§å¼  9")
    print("   æ„‰æ‚¦ 7")
    print("   å¹³é™ 5")
    print("   3é»„è‰³ä¸½")
    print("   æ„‰æ‚¦ 7")
    print("   æ„‰æ‚¦")
    print("   å¹³é™ 4")
    print("   æ„‰æ‚¦ 7")
    print("3. è„šæœ¬ä¼šè‡ªåŠ¨:")
    print("   - æ ¹æ®äººååŒ¹é…å¯¹åº”çš„CSVæ–‡ä»¶")
    print("   - å¯¹æ¯ä¸ªCSVæ–‡ä»¶è¿›è¡Œå³°å€¼æ£€æµ‹")
    print("   - ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆ40sé«˜è´¨é‡æ•°æ®æ®µ")
    print("   - å¯¹æ¯ä¸ªæ•°æ®æ®µè®¡ç®—HRVç‰¹å¾")
    print("   - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶")
    print("   - å°†ç»“æœè¿½åŠ åˆ° hrv_data_batch_40s.csv")
    print("   - å¤‡ä»½RRé—´éš”æ•°æ®åˆ°40sdata/backup_batchç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰")
    print("4. è¾“å‡ºæ ¼å¼: è°­æ€¡é›… å¹³é™, è°­æ€¡é›… æ„‰æ‚¦ 6, å”é“­é¥ ç´§å¼  9, ...")
    print("5. æ¯ä¸ªCSVæ–‡ä»¶å¯èƒ½äº§ç”Ÿå¤šä¸ªæ•°æ®æ®µï¼Œæ–‡ä»¶åæ ¼å¼: åŸæ–‡ä»¶å_seg1, åŸæ–‡ä»¶å_seg2...")
    print("6. RRæ•°æ®å¤‡ä»½: æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®å•ç‹¬ä¿å­˜åˆ°ä¸åŒç›®å½•ï¼Œæ–‡ä»¶ååŒ…å«_40sæ ‡è¯†")

if __name__ == "__main__":
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    show_usage_example()
    print("\n" + "="*60 + "\n")
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
    subprocess.run(["clear_processed_data.bat"])
