#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šçº¿ç¨‹HRVç‰¹å¾è®¡ç®—è„šæœ¬ - 40ç§’ç‰ˆæœ¬
åˆå¹¶äº†æ™ºèƒ½æ•°æ®æ®µå¤„ç†å’ŒHRVç‰¹å¾è®¡ç®—åŠŸèƒ½
æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†CSVæ–‡ä»¶
ä½¿ç”¨40ç§’æ»‘åŠ¨çª—å£æ£€æµ‹é«˜è´¨é‡æ•°æ®æ®µ
"""

import os
import math
import glob
import numpy as np
import pandas as pd
from scipy import signal
from scipy.stats import entropy, zscore
from scipy.signal import find_peaks
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict
import time
import sys
import subprocess

# --------------------------------------------------------------------------------------------
# é…ç½®åŒºåŸŸ - ç”¨æˆ·éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†
# --------------------------------------------------------------------------------------------

# è¾“å…¥æ•°æ®ç›®å½•ï¼ˆåŒ…å«å·²åˆ’åˆ†å¥½çš„CSVæ–‡ä»¶ï¼‰
INPUT_DATA_DIR = r"C:\Users\UiNCeY\Desktop\emotion\processed_data"

# è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE = r"C:\Users\QAQ\Desktop\emotion\hrv_data.csv"

# RRé—´éš”æ•°æ®å¤‡ä»½ç›®å½•ï¼ˆæ¯ä¸ªæ ‡ç­¾å•ç‹¬ä¿å­˜ï¼‰
RR_BACKUP_BASE_DIR = r"C:\Users\UiNCeY\Desktop\emotion\backup"  # 40ç§’ç‰ˆæœ¬å¤‡ä»½ç›®å½•

# æœ€å¤§çº¿ç¨‹æ•°ï¼ˆå»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„1-2å€ï¼Œå› ä¸ºHRVè®¡ç®—æ˜¯CPUå¯†é›†å‹ï¼‰
MAX_WORKERS = 14

# æ™ºèƒ½æ•°æ®æ®µå¤„ç†å‚æ•° - 40ç§’ç‰ˆæœ¬
SEGMENT_DURATION_MS = 10000  # æ•°æ®æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰- 40ç§’
PEAK_DETECTION_PARAMS = {
    'distance': 5,           # å³°å€¼é—´æœ€å°è·ç¦»
    'prominence': 25,        # å³°å€¼çªå‡ºåº¦
    'height': None           # å³°å€¼é«˜åº¦é˜ˆå€¼ï¼ˆNoneä¸ºè‡ªåŠ¨ï¼‰
}

# è´¨é‡è¯„ä¼°å‚æ•° - é’ˆå¯¹40ç§’æ•°æ®æ®µè°ƒæ•´
QUALITY_PARAMS = {
    'min_peaks_per_segment': 6,     # æ¯æ®µæœ€å°‘å³°å€¼æ•°ï¼ˆ40ç§’éœ€è¦æ›´å¤šå³°å€¼ï¼‰
    'max_peaks_per_segment': 20,     # æ¯æ®µæœ€å¤šå³°å€¼æ•°ï¼ˆ40ç§’å¯ä»¥å®¹çº³æ›´å¤šå³°å€¼ï¼‰
    'gap_threshold_factor': 2.5,     # æ–­ç‚¹æ£€æµ‹é˜ˆå€¼å› å­ï¼ˆå€æ•°ï¼‰
    'rr_variability_threshold': 0.3, # RRé—´æœŸå˜å¼‚æ€§é˜ˆå€¼
    'outlier_threshold': 2.5,        # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼ï¼ˆZ-scoreï¼‰
    'rr_range_min_factor': 0.7,      # RRé—´éš”èŒƒå›´ä¸‹é™å› å­ï¼ˆ70%ï¼‰
    'rr_range_max_factor': 1.3,      # RRé—´éš”èŒƒå›´ä¸Šé™å› å­ï¼ˆ130%ï¼‰
    'min_segment_quality_score': 1.1 # æœ€ä½è´¨é‡è¯„åˆ†
}

# --------------------------------------------------------------------------------------------
# æ™ºèƒ½æ•°æ®æ®µå¤„ç†æ ¸å¿ƒå‡½æ•°
# --------------------------------------------------------------------------------------------

def detect_all_peaks(data: pd.DataFrame) -> tuple:
    """å¯¹æ•´æ®µæ•°æ®è¿›è¡Œå³°å€¼æ£€æµ‹"""
    signal_values = data['æ•°å€¼'].values
    
    # æ‰§è¡Œå³°å€¼æ£€æµ‹
    peaks, properties = find_peaks(
        signal_values,
        distance=PEAK_DETECTION_PARAMS['distance'],
        prominence=PEAK_DETECTION_PARAMS['prominence'],
        height=PEAK_DETECTION_PARAMS['height']
    )
    
    if len(peaks) == 0:
        return None, None
    
    # è®¡ç®—å³°å€¼æ—¶é—´å’ŒRRé—´æœŸ
    peak_times = data['æ—¶é—´'].iloc[peaks].values
    rr_intervals = np.diff(peak_times)
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    peak_info = {
        'indices': peaks,
        'times': peak_times,
        'values': signal_values[peaks],
        'rr_intervals': rr_intervals,
        'mean_rr': np.mean(rr_intervals),
        'std_rr': np.std(rr_intervals)
    }
    
    return peak_info, properties

def detect_gaps_and_anomalies(peak_info: dict) -> np.ndarray:
    """æ£€æµ‹å³°å€¼é—´çš„æ–­ç‚¹å’Œå¼‚å¸¸"""
    if peak_info is None or len(peak_info['rr_intervals']) == 0:
        return np.array([])
    
    rr_intervals = peak_info['rr_intervals']
    mean_rr = peak_info['mean_rr']
    
    # æ–¹æ³•1: åŸºäºé˜ˆå€¼å› å­çš„æ–­ç‚¹æ£€æµ‹
    gap_threshold = mean_rr * QUALITY_PARAMS['gap_threshold_factor']
    gap_indices = np.where(rr_intervals > gap_threshold)[0]
    
    # æ–¹æ³•2: åŸºäºZ-scoreçš„å¼‚å¸¸æ£€æµ‹
    outlier_indices = np.array([])
    try:
        if len(rr_intervals) > 1 and np.std(rr_intervals) > 1e-10:
            z_scores = np.abs(zscore(rr_intervals))
            valid_z_mask = np.isfinite(z_scores)
            if np.any(valid_z_mask):
                outlier_indices = np.where((z_scores > QUALITY_PARAMS['outlier_threshold']) & valid_z_mask)[0]
    except Exception:
        outlier_indices = np.array([])
    
    # åˆå¹¶å¼‚å¸¸ä½ç½®
    if len(gap_indices) > 0 and len(outlier_indices) > 0:
        anomaly_indices = np.unique(np.concatenate([gap_indices, outlier_indices]))
    elif len(gap_indices) > 0:
        anomaly_indices = gap_indices
    elif len(outlier_indices) > 0:
        anomaly_indices = outlier_indices
    else:
        anomaly_indices = np.array([])
    
    return anomaly_indices

def evaluate_segment_quality(data_segment: pd.DataFrame, segment_peak_times: np.ndarray, 
                           segment_start_time: float, segment_end_time: float) -> tuple:
    """è¯„ä¼°å•ä¸ª40sæ•°æ®æ®µçš„è´¨é‡ï¼Œè¿”å›(æ˜¯å¦é€šè¿‡, è´¨é‡ä¿¡æ¯)"""
    
    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å³°å€¼æ—¶é—´
    segment_peaks = segment_peak_times[(segment_peak_times >= segment_start_time) & (segment_peak_times <= segment_end_time)]
    
    quality_score = 0.0
    issues = []
    
    # æ£€æŸ¥1: å³°å€¼æ•°é‡ - ç›´æ¥å‰”é™¤ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®æ®µ
    peak_count = len(segment_peaks)
    if peak_count < QUALITY_PARAMS['min_peaks_per_segment']:
        return False, {
            'quality_score': 0.0,
            'peak_count': peak_count,
            'issues': [f"å³°å€¼æ•°é‡ä¸è¶³({peak_count}<{QUALITY_PARAMS['min_peaks_per_segment']})"],
            'rr_mean': 0,
            'rr_std': 0,
            'completeness': 0
        }
    elif peak_count > QUALITY_PARAMS['max_peaks_per_segment']:
        return False, {
            'quality_score': 0.0,
            'peak_count': peak_count,
            'issues': [f"å³°å€¼æ•°é‡è¿‡å¤š({peak_count}>{QUALITY_PARAMS['max_peaks_per_segment']})"],
            'rr_mean': 0,
            'rr_std': 0,
            'completeness': 0
        }
    else:
        quality_score += 0.3  # å³°å€¼æ•°é‡åˆç†
    
    # æ£€æŸ¥2: RRé—´æœŸå˜å¼‚æ€§ - ç›´æ¥å‰”é™¤å˜å¼‚æ€§è¿‡é«˜çš„æ•°æ®æ®µ
    if len(segment_peaks) > 1:
        rr_intervals = np.diff(segment_peaks)
        mean_rr = np.mean(rr_intervals)
        std_rr = np.std(rr_intervals)
        cv_rr = std_rr / mean_rr if mean_rr > 0 else float('inf')
        
        if cv_rr > QUALITY_PARAMS['rr_variability_threshold']:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"RRé—´æœŸå˜å¼‚æ€§è¿‡é«˜(CV={cv_rr:.3f})"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.3  # å˜å¼‚æ€§åˆç†
        
        # æ£€æŸ¥3: æ–­ç‚¹æ£€æµ‹ - ç›´æ¥å‰”é™¤å­˜åœ¨æ–­ç‚¹çš„æ•°æ®æ®µ
        gap_threshold = mean_rr * QUALITY_PARAMS['gap_threshold_factor']
        gaps = np.sum(rr_intervals > gap_threshold)
        if gaps > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{gaps}ä¸ªæ–­ç‚¹"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ— æ–­ç‚¹
        
        # æ£€æŸ¥4: å¼‚å¸¸å€¼æ£€æµ‹ - ç›´æ¥å‰”é™¤å­˜åœ¨å¼‚å¸¸å€¼çš„æ•°æ®æ®µ
        z_scores = np.abs(zscore(rr_intervals))
        outliers = np.sum(z_scores > QUALITY_PARAMS['outlier_threshold'])
        if outliers > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{outliers}ä¸ªå¼‚å¸¸å€¼"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ— å¼‚å¸¸å€¼
        
        # æ£€æŸ¥5: RRé—´éš”èŒƒå›´æ£€æŸ¥ï¼ˆ70%-130%ï¼‰- ç›´æ¥å‰”é™¤è¶…å‡ºèŒƒå›´çš„æ•°æ®æ®µ
        rr_min_threshold = mean_rr * QUALITY_PARAMS['rr_range_min_factor']
        rr_max_threshold = mean_rr * QUALITY_PARAMS['rr_range_max_factor']
        out_of_range_count = np.sum((rr_intervals < rr_min_threshold) | (rr_intervals > rr_max_threshold))
        if out_of_range_count > 0:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å­˜åœ¨{out_of_range_count}ä¸ªRRé—´éš”è¶…å‡º70%-130%èŒƒå›´"],
                'rr_mean': mean_rr,
                'rr_std': std_rr,
                'completeness': 0
            }
        else:
            quality_score += 0.2  # æ‰€æœ‰RRé—´éš”åœ¨åˆç†èŒƒå›´å†…
    
    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    expected_points = int(SEGMENT_DURATION_MS * len(data_segment) / (data_segment['æ—¶é—´'].max() - data_segment['æ—¶é—´'].min()))
    actual_points = len(data_segment)
    completeness = min(1.0, actual_points / expected_points)
    quality_score += 0.2 * completeness
    
    # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ï¼Œè¿”å›é€šè¿‡çŠ¶æ€
    return True, {
        'quality_score': quality_score,
        'peak_count': peak_count,
        'issues': [],
        'rr_mean': np.mean(rr_intervals) if len(segment_peaks) > 1 else 0,
        'rr_std': np.std(rr_intervals) if len(segment_peaks) > 1 else 0,
        'completeness': completeness
    }

def generate_candidate_segments(data: pd.DataFrame, peak_info: dict) -> list:
    """ç”Ÿæˆå€™é€‰çš„40sæ•°æ®æ®µ - åŸºäºå³°å€¼ä½ç½®çš„æ»‘åŠ¨çª—å£"""
    
    if peak_info is None or len(peak_info['times']) < 2:
        return []
    
    # æ£€æµ‹å¼‚å¸¸ä½ç½®
    anomaly_indices = detect_gaps_and_anomalies(peak_info)
    
    # è·å–æ‰€æœ‰å³°å€¼æ—¶é—´
    peak_times = peak_info['times']
    data_start = data['æ—¶é—´'].min()
    data_end = data['æ—¶é—´'].max()
    
    candidates = []
    
    # ä»æ¯ä¸ªå³°å€¼å¼€å§‹ï¼Œå°è¯•ç”Ÿæˆ40sçš„æ•°æ®æ®µ
    for i, start_peak_time in enumerate(peak_times):
        # è®¡ç®—çª—å£ç»“æŸæ—¶é—´
        end_time = start_peak_time + SEGMENT_DURATION_MS
        
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ•°æ®èŒƒå›´
        if end_time > data_end:
            break
        
        # æå–æ•°æ®æ®µ
        mask = (data['æ—¶é—´'] >= start_peak_time) & (data['æ—¶é—´'] <= end_time)
        segment_data = data[mask].copy().reset_index(drop=True)
        
        if len(segment_data) < 200:  # 40ç§’éœ€è¦æ›´å¤šæ•°æ®ç‚¹
            continue
        
        # æ‰¾åˆ°è¯¥æ®µå†…çš„å³°å€¼æ—¶é—´
        segment_peak_times = peak_times[(peak_times >= start_peak_time) & (peak_times <= end_time)]
        
        # è¯„ä¼°è´¨é‡ - ç›´æ¥å‰”é™¤æœ‰é—®é¢˜çš„æ•°æ®æ®µ
        quality_passed, quality = evaluate_segment_quality(segment_data, segment_peak_times, start_peak_time, end_time)
        
        # åªä¿ç•™é€šè¿‡è´¨é‡æ£€æŸ¥çš„æ®µ
        if quality_passed:
            candidates.append({
                'start_time': start_peak_time,
                'end_time': end_time,
                'start_peak_index': i,  # è®°å½•èµ·å§‹å³°å€¼ç´¢å¼•
                'data': segment_data,
                'peak_times': segment_peak_times,
                'quality': quality
            })
    
    # æŒ‰è´¨é‡è¯„åˆ†æ’åº
    candidates.sort(key=lambda x: x['quality']['quality_score'], reverse=True)
    
    # å»é‡ï¼šå¦‚æœä¸¤ä¸ªå€™é€‰æ®µé‡å åº¦å¾ˆé«˜ï¼Œåªä¿ç•™è´¨é‡æ›´é«˜çš„
    filtered_candidates = []
    for candidate in candidates:
        is_duplicate = False
        for existing in filtered_candidates:
            # è®¡ç®—é‡å åº¦
            overlap_start = max(candidate['start_time'], existing['start_time'])
            overlap_end = min(candidate['end_time'], existing['end_time'])
            overlap_duration = max(0, overlap_end - overlap_start)
            overlap_ratio = overlap_duration / SEGMENT_DURATION_MS
            
            # å¦‚æœé‡å è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
            if overlap_ratio > 0.7:
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered_candidates.append(candidate)
    
    return filtered_candidates

# --------------------------------------------------------------------------------------------
# HRVç‰¹å¾è®¡ç®—æ ¸å¿ƒå‡½æ•°
# --------------------------------------------------------------------------------------------

def read_rr_data(path: str) -> np.ndarray:
    """è¯»å–RRé—´æœŸæ•°æ®ï¼ˆæ¯«ç§’ï¼‰"""
    data = pd.read_csv(path, header=None)
    if data.shape[1] < 1:
        raise ValueError("RRæ–‡ä»¶éœ€è¦è‡³å°‘ä¸€åˆ—æ•°æ®")
    rr_ms = data.iloc[:, 0].to_numpy(dtype=float)
    
    return rr_ms

def calculate_hrv_features(rr_ms: np.ndarray) -> dict:
    """è®¡ç®—æŒ‡å®š7é¡¹HRVç‰¹å¾ã€‚è¾“å…¥å•ä½: ms - ä¼˜åŒ–ç‰ˆæœ¬"""
    if rr_ms is None or len(rr_ms) < 2:
        return {}

    feat = {}

    # --- æ—¶åŸŸç‰¹å¾ - å‘é‡åŒ–è®¡ç®— ---
    rr_diff = np.diff(rr_ms)
    
    # RMSSD - å‘é‡åŒ–è®¡ç®—
    feat['RMSSD'] = float(np.sqrt(np.mean(rr_diff ** 2))) if len(rr_diff) > 0 else np.nan
    
    # pNN58 - å‘é‡åŒ–è®¡ç®—
    if len(rr_diff) > 0:
        over_58_count = np.sum(np.abs(rr_diff) > 58.0)
        feat['pNN58'] = float((over_58_count / len(rr_diff)) * 100.0)
    else:
        feat['pNN58'] = np.nan
    
    # SDNN - å‘é‡åŒ–è®¡ç®—
    feat['SDNN'] = float(np.std(rr_ms, ddof=1)) if len(rr_ms) > 1 else np.nan

    # --- PoincarÃ©ç‰¹å¾ - å‘é‡åŒ–è®¡ç®— ---
    if len(rr_diff) > 0:
        var_diff = np.var(rr_diff)
        var_rr = np.var(rr_ms, ddof=1)
        
        sd1 = float(np.sqrt(var_diff / 2.0))
        sd2 = float(np.sqrt(2 * var_rr - var_diff / 2.0))
        
        feat['SD1'] = sd1
        feat['SD2'] = sd2
        feat['SD1_SD2'] = float(sd1 / sd2) if sd2 > 0 else np.nan
    else:
        feat['SD1'] = np.nan
        feat['SD2'] = np.nan
        feat['SD1_SD2'] = np.nan

    # --- SampEn --- é«˜æ•ˆç‰ˆæœ¬ï¼Œä½¿ç”¨è¿‘ä¼¼ç®—æ³•
    sampen = np.nan
    try:
        if len(rr_ms) >= 10:
            # ä½¿ç”¨æ›´å¿«çš„è¿‘ä¼¼ç®—æ³•
            m = 2
            r = 0.2 * np.std(rr_ms)
            
            # é™åˆ¶æ•°æ®é•¿åº¦ä»¥æé«˜æ€§èƒ½
            max_len = min(len(rr_ms), 500)  # è¿›ä¸€æ­¥é™åˆ¶é•¿åº¦
            rr_subset = rr_ms[:max_len]
            
            # ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•
            count_m = 0
            count_m1 = 0
            total_m = 0
            total_m1 = 0
            
            # é¢„è®¡ç®—æ‰€æœ‰çª—å£
            windows_m = np.array([rr_subset[i:i+m] for i in range(len(rr_subset) - m)])
            windows_m1 = np.array([rr_subset[i:i+m+1] for i in range(len(rr_subset) - m - 1)])
            
            # å‘é‡åŒ–æ¯”è¾ƒ
            for i in range(len(windows_m)):
                # é™åˆ¶æ¯”è¾ƒèŒƒå›´ä»¥æé«˜æ€§èƒ½
                end_idx = min(i + 30, len(windows_m))
                
                for j in range(i + 1, end_idx):
                    if np.max(np.abs(windows_m[i] - windows_m[j])) <= r:
                        count_m += 1
                    total_m += 1
                    
                    if i < len(windows_m1) and j < len(windows_m1):
                        if np.max(np.abs(windows_m1[i] - windows_m1[j])) <= r:
                            count_m1 += 1
                        total_m1 += 1
            
            if count_m > 0 and count_m1 > 0 and total_m > 0 and total_m1 > 0:
                sampen = -np.log((count_m1 / total_m1) / (count_m / total_m))
    except Exception:
        pass
    feat['SampEn'] = sampen

    return feat

# --------------------------------------------------------------------------------------------
# å¤šçº¿ç¨‹å¤„ç†ç±»
# --------------------------------------------------------------------------------------------

class MultiThreadHRVProcessor:
    def __init__(self, input_dir: str, output_file: str, max_workers: int = 8):
        self.input_dir = input_dir
        self.output_file = output_file
        self.max_workers = max_workers
        self.lock = threading.Lock()  # ç”¨äºä¿æŠ¤æ–‡ä»¶å†™å…¥æ“ä½œ
        self.results = []  # å­˜å‚¨æ‰€æœ‰å¤„ç†ç»“æœ
        self.processing_status = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†çŠ¶æ€
        self.thread_status = {}  # å­˜å‚¨æ¯ä¸ªçº¿ç¨‹çš„çŠ¶æ€
        self.monitor_running = False  # ç›‘æ§çº¿ç¨‹è¿è¡ŒçŠ¶æ€
        self.performance_stats = {
            'start_time': 0,
            'processed_files': 0,
            'total_files': 0,
            'cpu_usage': 0,
            'memory_usage': 0
        }
        self.rr_backup_enabled = bool(RR_BACKUP_BASE_DIR.strip())
        self.rr_backup_dirs = {}  # å­˜å‚¨æ¯ä¸ªæ ‡ç­¾çš„å¤‡ä»½ç›®å½•
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file) if os.path.dirname(output_file) else "."
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
    
    def process_single_csv(self, csv_file: str, emotion_label: str, thread_id: int) -> pd.DataFrame:
        """å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼Œå…ˆè¿›è¡Œæ™ºèƒ½æ•°æ®æ®µå¤„ç†ï¼Œç„¶åè®¡ç®—HRVç‰¹å¾"""
        filename = os.path.basename(csv_file)
        
        try:
            # æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼Œå‡å°‘é”æ“ä½œ
            self._update_status_batch(thread_id, filename, "è¯»å–ä¸­")
            
            # è¯»å–æ•°æ®
            data = pd.read_csv(csv_file, header=None)
            
            if data.shape[1] < 2:
                self._update_status_batch(thread_id, filename, "æ ¼å¼é”™è¯¯")
                print(f"\nâŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {filename} (éœ€è¦è‡³å°‘2åˆ—æ•°æ®)")
                return pd.DataFrame()
            
            # è®¾ç½®åˆ—å
            data = data.iloc[:, :2].copy()
            data.columns = ['æ—¶é—´', 'æ•°å€¼']
            
            if len(data) < 200:  # 40ç§’éœ€è¦æ›´å¤šæ•°æ®ç‚¹
                self._update_status_batch(thread_id, filename, "æ•°æ®ä¸è¶³")
                print(f"\nâŒ æ•°æ®ä¸è¶³: {filename} (éœ€è¦è‡³å°‘200ä¸ªæ•°æ®ç‚¹)")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "å³°å€¼æ£€æµ‹ä¸­")
            
            # ç¬¬1æ­¥ï¼šå³°å€¼æ£€æµ‹
            peak_info, _ = detect_all_peaks(data)
            if peak_info is None:
                self._update_status_batch(thread_id, filename, "å³°å€¼æ£€æµ‹å¤±è´¥")
                print(f"\nâŒ å³°å€¼æ£€æµ‹å¤±è´¥: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "ç”Ÿæˆæ•°æ®æ®µä¸­")
            
            # ç¬¬2æ­¥ï¼šç”Ÿæˆå€™é€‰æ•°æ®æ®µ
            candidates = generate_candidate_segments(data, peak_info)
            if not candidates:
                self._update_status_batch(thread_id, filename, "æ— é«˜è´¨é‡æ•°æ®æ®µ")
                print(f"\nâŒ æ— é«˜è´¨é‡æ•°æ®æ®µ: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, "è®¡ç®—HRVç‰¹å¾ä¸­")
            
            # ç¬¬3æ­¥ï¼šå¯¹æ¯ä¸ªé«˜è´¨é‡æ•°æ®æ®µè®¡ç®—HRVç‰¹å¾
            results = []
            for i, segment in enumerate(candidates):
                # ä»æ•°æ®æ®µä¸­æå–RRé—´æœŸ
                if len(segment['peak_times']) > 1:
                    rr_intervals = np.diff(segment['peak_times'])
                    
                    # å¤‡ä»½RRé—´éš”æ•°æ®
                    backup_path = self._backup_rr_data(rr_intervals, filename, emotion_label, i+1)
                    
                    # è®¡ç®—HRVç‰¹å¾
                    feat = calculate_hrv_features(rr_intervals)
                    
                    if feat:
                        # æ„å»ºç»“æœè¡Œ
                        row = {
                            'file': f"{filename}_seg{i+1}",
                            'RMSSD': feat.get('RMSSD', np.nan),
                            'pNN58': feat.get('pNN58', np.nan),
                            'SDNN': feat.get('SDNN', np.nan),
                            'SD1': feat.get('SD1', np.nan),
                            'SD2': feat.get('SD2', np.nan),
                            'SD1_SD2': feat.get('SD1_SD2', np.nan),
                            'SampEn': feat.get('SampEn', np.nan),
                            'emotion': emotion_label
                        }
                        results.append(row)
            
            if not results:
                self._update_status_batch(thread_id, filename, "HRVè®¡ç®—å¤±è´¥")
                print(f"\nâŒ HRVç‰¹å¾è®¡ç®—å¤±è´¥: {filename}")
                return pd.DataFrame()
            
            # æ‰¹é‡æ›´æ–°çŠ¶æ€
            self._update_status_batch(thread_id, filename, f"å·²å®Œæˆ({len(results)}æ®µ)")
            
            return pd.DataFrame(results)
            
        except Exception as e:
            self._update_status_batch(thread_id, filename, "å¤„ç†å¤±è´¥")
            print(f"\nâŒ å¤„ç†å¤±è´¥: {filename} -> {e}")
            return pd.DataFrame()
    
    def _update_status_batch(self, thread_id: int, filename: str, status: str):
        """æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼Œå‡å°‘é”æ“ä½œé¢‘ç‡"""
        with self.lock:
            self.thread_status[thread_id] = f"{status} {filename}"
            self.processing_status[filename] = status
    
    def _get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_percent = psutil.virtual_memory().percent
            return cpu_percent, memory_percent
        except:
            return 0, 0
    
    def _update_performance_stats(self, processed_count: int):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            self.performance_stats['processed_files'] = processed_count
            cpu_usage, memory_usage = self._get_performance_stats()
            self.performance_stats['cpu_usage'] = cpu_usage
            self.performance_stats['memory_usage'] = memory_usage
    
    def _setup_rr_backup_dir(self, emotion_label: str) -> str:
        """ä¸ºæ¯ä¸ªæ ‡ç­¾è®¾ç½®RRæ•°æ®å¤‡ä»½ç›®å½•"""
        if not self.rr_backup_enabled:
            return None
        
        # æ¸…ç†æ ‡ç­¾åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_label = emotion_label.replace(" ", "_").replace("/", "_").replace("\\", "_")
        backup_dir = os.path.join(RR_BACKUP_BASE_DIR, clean_label)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆä½¿ç”¨exist_ok=Trueé¿å…ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯ï¼‰
        try:
            os.makedirs(backup_dir, exist_ok=True)
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {backup_dir}, é”™è¯¯: {e}")
            return None
        
        return backup_dir
    
    def _backup_rr_data(self, rr_intervals: np.ndarray, filename: str, emotion_label: str, segment_index: int) -> str:
        """å¤‡ä»½RRé—´éš”æ•°æ®åˆ°æŒ‡å®šç›®å½•"""
        if not self.rr_backup_enabled:
            return None
        
        # è·å–æˆ–åˆ›å»ºå¤‡ä»½ç›®å½•
        if emotion_label not in self.rr_backup_dirs:
            self.rr_backup_dirs[emotion_label] = self._setup_rr_backup_dir(emotion_label)
        
        backup_dir = self.rr_backup_dirs[emotion_label]
        if not backup_dir:
            return None
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        base_name = os.path.splitext(filename)[0]
        backup_filename = f"{base_name}_seg{segment_index}_RR_40s.csv"
        backup_path = os.path.join(backup_dir, backup_filename)
        
        try:
            # ä¿å­˜RRé—´éš”æ•°æ®
            pd.DataFrame(rr_intervals).to_csv(backup_path, index=False, header=False)
            return backup_path
        except Exception as e:
            print(f"\nâš ï¸ RRæ•°æ®å¤‡ä»½å¤±è´¥: {backup_path} - {e}")
            return None
    
    def process_all_files(self, csv_files: List[str], emotion_labels: List[str]) -> None:
        """å¤šçº¿ç¨‹å¤„ç†æ‰€æœ‰CSVæ–‡ä»¶"""
        if len(csv_files) != len(emotion_labels):
            print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶æ•°é‡({len(csv_files)})ä¸æ ‡ç­¾æ•°é‡({len(emotion_labels)})ä¸åŒ¹é…")
            return
        
        print(f"ğŸ“Š å¼€å§‹å¤šçº¿ç¨‹å¤„ç† {len(csv_files)} ä¸ªæ–‡ä»¶ (40ç§’æ•°æ®æ®µ)")
        print(f"ğŸ”§ ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        print()
        
        # é»˜è®¤å¯ç”¨å®æ—¶ç›‘æ§
        monitor_thread = self._start_status_monitor()
        print("âœ… å®æ—¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C å¯åœæ­¢ç›‘æ§")
        time.sleep(1)
        
        start_time = time.time()
        success_count = 0
        
        # åˆå§‹åŒ–æ€§èƒ½ç»Ÿè®¡
        self.performance_stats['start_time'] = start_time
        self.performance_stats['total_files'] = len(csv_files)
        
        # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
        for csv_file in csv_files:
            self.processing_status[os.path.basename(csv_file)] = "ç­‰å¾…ä¸­"
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…çº¿ç¨‹ID
            future_to_file = {}
            for i, (csv_file, emotion_label) in enumerate(zip(csv_files, emotion_labels)):
                future = executor.submit(self.process_single_csv, csv_file, emotion_label, i % self.max_workers)
                future_to_file[future] = (csv_file, emotion_label)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            completed_count = 0
            for future in as_completed(future_to_file):
                csv_file, emotion_label = future_to_file[future]
                filename = os.path.basename(csv_file)
                
                try:
                    result = future.result()
                    if not result.empty:
                        with self.lock:
                            self.results.append(result)
                        success_count += 1
                        self.processing_status[filename] = "âœ… æˆåŠŸ"
                    else:
                        self.processing_status[filename] = "âŒ å¤±è´¥"
                except Exception as e:
                    print(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {filename} - {e}")
                    self.processing_status[filename] = "âŒ å¼‚å¸¸"
                
                completed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if not self.monitor_running:  # å¦‚æœæ²¡æœ‰å¯ç”¨å®æ—¶ç›‘æ§ï¼Œæ˜¾ç¤ºç®€å•è¿›åº¦
                    progress = (completed_count / len(csv_files)) * 100
                    # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                    self._update_performance_stats(completed_count)
                    
                    # è®¡ç®—å¤„ç†é€Ÿåº¦
                    elapsed_time = time.time() - start_time
                    files_per_second = completed_count / elapsed_time if elapsed_time > 0 else 0
                    
                    print(f"\rğŸ”„ å¤„ç†è¿›åº¦: {completed_count}/{len(csv_files)} ({progress:.1f}%) - æˆåŠŸ: {success_count}, å¤±è´¥: {completed_count - success_count} - é€Ÿåº¦: {files_per_second:.1f} æ–‡ä»¶/ç§’ - CPU: {self.performance_stats['cpu_usage']:.1f}%", end="", flush=True)
        
        end_time = time.time()
        
        # åœæ­¢ç›‘æ§
        if monitor_thread:
            self._stop_status_monitor()
            time.sleep(1)  # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"æ€»æ–‡ä»¶æ•°: {len(csv_files)}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤±è´¥æ–‡ä»¶: {len(csv_files) - success_count}")
        print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’")
        
        # æ˜¾ç¤ºè¯¦ç»†å¤„ç†ç»“æœ
        self._show_detailed_results()
    
    def _show_detailed_results(self):
        """æ˜¾ç¤ºè¯¦ç»†å¤„ç†ç»“æœ"""
        print(f"\nğŸ“‹ è¯¦ç»†å¤„ç†ç»“æœ:")
        print("-" * 80)
        
        # æŒ‰çŠ¶æ€åˆ†ç»„æ˜¾ç¤º
        status_groups = {}
        for filename, status in self.processing_status.items():
            if status not in status_groups:
                status_groups[status] = []
            status_groups[status].append(filename)
        
        for status, files in status_groups.items():
            print(f"{status}: {len(files)} ä¸ªæ–‡ä»¶")
            for filename in sorted(files):
                print(f"  - {filename}")
            print()
    
    def _start_status_monitor(self):
        """å¯åŠ¨çŠ¶æ€ç›‘æ§çº¿ç¨‹"""
        def monitor():
            while self.monitor_running:
                with self.lock:
                    # æ¸…å±å¹¶æ˜¾ç¤ºå½“å‰çŠ¶æ€
                    os.system('cls' if os.name == 'nt' else 'clear')
                    print("ğŸ”„ å®æ—¶å¤„ç†çŠ¶æ€ç›‘æ§ (40ç§’æ•°æ®æ®µ)")
                    print("=" * 60)
                    
                    # æ˜¾ç¤ºçº¿ç¨‹çŠ¶æ€
                    print("ğŸ§µ çº¿ç¨‹çŠ¶æ€:")
                    for thread_id in range(self.max_workers):
                        status = self.thread_status.get(thread_id, "ç©ºé—²")
                        print(f"  çº¿ç¨‹ {thread_id}: {status}")
                    
                    print("\nğŸ“ æ–‡ä»¶å¤„ç†çŠ¶æ€:")
                    # æŒ‰çŠ¶æ€åˆ†ç»„æ˜¾ç¤º
                    status_groups = {}
                    for filename, status in self.processing_status.items():
                        if status not in status_groups:
                            status_groups[status] = []
                        status_groups[status].append(filename)
                    
                    for status, files in status_groups.items():
                        print(f"  {status}: {len(files)} ä¸ªæ–‡ä»¶")
                        for filename in sorted(files)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            print(f"    - {filename}")
                        if len(files) > 5:
                            print(f"    ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
                    
                    print(f"\nâ±ï¸  æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
                
                time.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
        
        self.monitor_running = True
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        return monitor_thread
    
    def _stop_status_monitor(self):
        """åœæ­¢çŠ¶æ€ç›‘æ§"""
        self.monitor_running = False
    
    def save_results(self) -> None:
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        df = pd.concat(self.results, ignore_index=True)
        
        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
        cols = ['file', 'RMSSD', 'pNN58', 'SDNN', 'SD1', 'SD2', 'SD1_SD2', 'SampEn', 'emotion']
        for c in cols:
            if c not in df.columns:
                df[c] = np.nan
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        df = df[cols]
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯æ–°å»º
        file_exists = os.path.exists(self.output_file)
        
        if file_exists:
            # è¿½åŠ æ¨¡å¼ï¼šä¸å†™è¡¨å¤´ï¼Œåªè¿½åŠ æ•°æ®è¡Œ
            df.to_csv(self.output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ æ–°è®°å½•å·²è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶: {self.output_file}")
        else:
            # æ–°å»ºæ¨¡å¼ï¼šå†™è¡¨å¤´å’Œæ•°æ®
            df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ HRVç‰¹å¾æ•°æ®é›†å·²åˆ›å»º: {self.output_file}")
        
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {df.shape}")
        print(f"ğŸ”¢ ç‰¹å¾æ•°é‡: {len(df.columns) - 2}")  # å‡å»fileå’Œemotionåˆ—
        print(f"ğŸ“ˆ æ ·æœ¬æ•°é‡: {len(df)}")
        
        # æ˜¾ç¤ºç‰¹å¾åˆ—ä¿¡æ¯
        print(f"ğŸ·ï¸ ç‰¹å¾åˆ—: {list(df.columns[1:-1])}")  # é™¤äº†fileå’Œemotion
        print(f"ğŸ·ï¸ æ ‡ç­¾åˆ—: {df.columns[-1]}")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
        print(f"\nğŸ“‹ æ•°æ®é›†å‰3è¡Œ:")
        print(df.head(3))
        
        # æ˜¾ç¤ºRRæ•°æ®å¤‡ä»½ä¿¡æ¯
        if self.rr_backup_enabled and self.rr_backup_dirs:
            print(f"\nğŸ’¾ RRé—´éš”æ•°æ®å¤‡ä»½ä¿¡æ¯ (40ç§’ç‰ˆæœ¬):")
            for emotion_label, backup_dir in self.rr_backup_dirs.items():
                if backup_dir and os.path.exists(backup_dir):
                    backup_files = glob.glob(os.path.join(backup_dir, "*_RR_40s.csv"))
                    print(f"   {emotion_label}: {len(backup_files)} ä¸ªæ–‡ä»¶ -> {backup_dir}")
        elif self.rr_backup_enabled:
            print(f"\nâš ï¸ RRæ•°æ®å¤‡ä»½åŠŸèƒ½å·²å¯ç”¨ï¼Œä½†æœªè®¾ç½®å¤‡ä»½ç›®å½•è·¯å¾„")
        else:
            print(f"\nğŸ’¾ RRæ•°æ®å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")

# --------------------------------------------------------------------------------------------
# æ ‡ç­¾è¾“å…¥å¤„ç†
# --------------------------------------------------------------------------------------------

def get_emotion_labels() -> List[str]:
    """ä»ç”¨æˆ·è¾“å…¥è·å–æƒ…ç»ªæ ‡ç­¾åˆ—è¡¨"""
    print("è¯·è¾“å…¥æ ‡ç­¾åˆ—è¡¨ï¼ˆç¬¬ä¸€ä¸ªæ˜¯å§“åï¼Œåç»­æ˜¯æƒ…ç»ªæ ‡ç­¾ï¼‰:")
    print("æ ¼å¼ç¤ºä¾‹:")
    print("å¼ ä¸‰")
    print("æ„‰æ‚¦")
    print("å¹³é™")
    print("ç„¦è™‘")
    print("æ‚²ä¼¤")
    print("...")
    print("è¾“å…¥å®ŒæˆåæŒ‰Ctrl+Zç„¶åå›è½¦ç»“æŸ")
    print()
    
    labels = []
    try:
        while True:
            line = input().strip()
            if line:  # å¿½ç•¥ç©ºè¡Œ
                labels.append(line)
    except EOFError:
        pass
    
    return labels

# --------------------------------------------------------------------------------------------
# ä¸»å¤„ç†å‡½æ•°
# --------------------------------------------------------------------------------------------

def main():
    print("=== å¤šçº¿ç¨‹HRVç‰¹å¾è®¡ç®—è„šæœ¬ - 40ç§’ç‰ˆæœ¬ ===")
    print(f"è¾“å…¥æ•°æ®ç›®å½•: {INPUT_DATA_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}")
    print(f"æ•°æ®æ®µé•¿åº¦: {SEGMENT_DURATION_MS/1000:.1f} ç§’")
    
    # æ˜¾ç¤ºRRå¤‡ä»½åŠŸèƒ½çŠ¶æ€
    if RR_BACKUP_BASE_DIR.strip():
        print(f"RRæ•°æ®å¤‡ä»½: å·²å¯ç”¨ -> {RR_BACKUP_BASE_DIR}")
    else:
        print(f"RRæ•°æ®å¤‡ä»½: æœªå¯ç”¨ (è¯·åœ¨è„šæœ¬ä¸­è®¾ç½® RR_BACKUP_BASE_DIR)")
    print()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(INPUT_DATA_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DATA_DIR}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = sorted(glob.glob(os.path.join(INPUT_DATA_DIR, "*.csv")))
    if not csv_files:
        print(f"âŒ åœ¨ {INPUT_DATA_DIR} ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")
    for i, csv_file in enumerate(csv_files, 1):
        print(f"   {i}. {os.path.basename(csv_file)}")
    print()
    
    # è·å–æ ‡ç­¾åˆ—è¡¨
    labels = get_emotion_labels()
    
    if not labels:
        print("âŒ æœªè¾“å…¥ä»»ä½•æ ‡ç­¾")
        return
    
    if len(labels) < 2:
        print("âŒ è‡³å°‘éœ€è¦è¾“å…¥å§“åå’Œä¸€ä¸ªæƒ…ç»ªæ ‡ç­¾")
        return
    
    # æ£€æŸ¥æ ‡ç­¾æ•°é‡æ˜¯å¦åŒ¹é…
    if len(labels) - 1 != len(csv_files):
        print(f"âŒ æ ‡ç­¾æ•°é‡ä¸åŒ¹é…:")
        print(f"   CSVæ–‡ä»¶æ•°é‡: {len(csv_files)}")
        print(f"   æƒ…ç»ªæ ‡ç­¾æ•°é‡: {len(labels) - 1}")
        print(f"   è¯·ç¡®ä¿æƒ…ç»ªæ ‡ç­¾æ•°é‡ä¸CSVæ–‡ä»¶æ•°é‡ä¸€è‡´")
        return
    
    # æ„å»ºå®Œæ•´çš„æƒ…ç»ªæ ‡ç­¾ï¼ˆå§“å + æƒ…ç»ªï¼‰
    name = labels[0]
    emotion_labels = [f"{name} {emotion}" for emotion in labels[1:]]
    
    print(f"ğŸ‘¤ å§“å: {name}")
    print(f"ğŸ·ï¸ æƒ…ç»ªæ ‡ç­¾:")
    for i, emotion_label in enumerate(emotion_labels, 1):
        print(f"   {i}. {emotion_label}")
    print()
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶å¼€å§‹å¤„ç†
    processor = MultiThreadHRVProcessor(INPUT_DATA_DIR, OUTPUT_FILE, MAX_WORKERS)
    processor.process_all_files(csv_files, emotion_labels)
    
    # ä¿å­˜ç»“æœ
    processor.save_results()
    
    print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")

# --------------------------------------------------------------------------------------------
# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
# --------------------------------------------------------------------------------------------

def show_usage_example():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("=== ä½¿ç”¨ç¤ºä¾‹ - 40ç§’ç‰ˆæœ¬ ===")
    print("1. ç¡®ä¿ processed_data ç›®å½•ä¸‹æœ‰å·²åˆ’åˆ†å¥½çš„CSVæ–‡ä»¶")
    print("2. è¿è¡Œè„šæœ¬åæŒ‰æç¤ºè¾“å…¥æ ‡ç­¾:")
    print("   å¼ ä¸‰")
    print("   æ„‰æ‚¦")
    print("   å¹³é™")
    print("   ç„¦è™‘")
    print("   æ‚²ä¼¤")
    print("3. è„šæœ¬ä¼šè‡ªåŠ¨:")
    print("   - å¯¹æ¯ä¸ªCSVæ–‡ä»¶è¿›è¡Œå³°å€¼æ£€æµ‹")
    print("   - ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆ40sé«˜è´¨é‡æ•°æ®æ®µ")
    print("   - å¯¹æ¯ä¸ªæ•°æ®æ®µè®¡ç®—HRVç‰¹å¾")
    print("   - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶")
    print("   - å°†ç»“æœè¿½åŠ åˆ° hrv_data_40s.csv")
    print("   - å¤‡ä»½RRé—´éš”æ•°æ®åˆ°40sdata/backupç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰")
    print("4. è¾“å‡ºæ ¼å¼: å¼ ä¸‰ æ„‰æ‚¦, å¼ ä¸‰ å¹³é™, å¼ ä¸‰ ç„¦è™‘, å¼ ä¸‰ æ‚²ä¼¤")
    print("5. æ¯ä¸ªCSVæ–‡ä»¶å¯èƒ½äº§ç”Ÿå¤šä¸ªæ•°æ®æ®µï¼Œæ–‡ä»¶åæ ¼å¼: åŸæ–‡ä»¶å_seg1, åŸæ–‡ä»¶å_seg2...")
    print("6. RRæ•°æ®å¤‡ä»½: æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®å•ç‹¬ä¿å­˜åˆ°ä¸åŒç›®å½•ï¼Œæ–‡ä»¶ååŒ…å«_40sæ ‡è¯†")

if __name__ == "__main__":
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    show_usage_example()
    print("\n" + "="*60 + "\n")
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
    subprocess.run(["clear_processed_data.bat"])