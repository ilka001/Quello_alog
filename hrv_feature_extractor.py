#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HRVç‰¹å¾æå–å™¨ - 10ç§’ç‰ˆæœ¬
ä»ä»»æ„é•¿åº¦çš„CSVæ–‡ä»¶ä¸­æå–HRVç‰¹å¾
è¾“å…¥: åŒ…å«æ—¶é—´å’Œæ•°å€¼ä¸¤åˆ—çš„CSVæ–‡ä»¶
è¾“å‡º: ä¸€è¡ŒåŒ…å«6ä¸ªHRVç‰¹å¾çš„æ•°æ®

ç‰¹ç‚¹:
- 10ç§’ç‰ˆæœ¬ï¼Œé€‚åˆçŸ­æ—¶é—´æ•°æ®åˆ†æ
- å–æ¶ˆè´¨é‡è¯„åˆ†æœºåˆ¶ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®
- åªè¦æ£€æµ‹åˆ°å³°å€¼å°±è®¡ç®—ç‰¹å¾ï¼Œä¸è®ºæ•°æ®è´¨é‡å¥½å

ä½¿ç”¨æ–¹æ³•:
1. ä¿®æ”¹ä»£ç é¡¶éƒ¨çš„é…ç½®å‚æ•°ï¼ˆæ¨èï¼‰
2. è¿è¡Œ: python hrv_feature_extractor.py --use-config
3. æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: python hrv_feature_extractor.py input.csv -o output.csv -e "æƒ…ç»ªæ ‡ç­¾"
"""

# ============================================================================================
# é…ç½®å‚æ•° - ç”¨æˆ·å¯ä¿®æ”¹çš„éƒ¨åˆ†
# ============================================================================================

# è¾“å…¥æ–‡ä»¶è·¯å¾„ - ç”¨æˆ·å¯åœ¨æ­¤å¤„ç›´æ¥ä¿®æ”¹è¦å¤„ç†çš„CSVæ–‡ä»¶è·¯å¾„
INPUT_CSV_PATH = r"C:\Users\QAQ\Desktop\emotion\spe d\hrv_19700101_000656.csv"  # ä¿®æ”¹ä¸ºä½ çš„CSVæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚: "data/heart_rate.csv"

# è¾“å‡ºæ–‡ä»¶è·¯å¾„ - ç”¨æˆ·å¯åœ¨æ­¤å¤„ç›´æ¥ä¿®æ”¹è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
OUTPUT_CSV_PATH = "hrv_features.csv"  # ä¿®æ”¹ä¸ºä½ çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚: "results/features.csv"

# æƒ…ç»ªæ ‡ç­¾ - ç”¨æˆ·å¯åœ¨æ­¤å¤„ç›´æ¥ä¿®æ”¹æƒ…ç»ªæ ‡ç­¾
EMOTION_LABEL = "å¹³é™"  # ä¿®æ”¹ä¸ºä½ çš„æƒ…ç»ªæ ‡ç­¾ï¼Œä¾‹å¦‚: "æ„‰æ‚¦"ã€"ç„¦è™‘"ã€"æ‚²ä¼¤"ç­‰

# æ•°æ®æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰- ç”¨æˆ·å¯æ ¹æ®éœ€è¦ä¿®æ”¹æ­¤å‚æ•°
SEGMENT_DURATION_MS = 10000  # 10ç§’ç‰ˆæœ¬ï¼Œå¯ä¿®æ”¹ä¸º5000(5ç§’)ã€15000(15ç§’)ç­‰

# å³°å€¼æ£€æµ‹å‚æ•° - ç”¨æˆ·å¯æ ¹æ®ä¿¡å·è´¨é‡è°ƒæ•´
PEAK_DETECTION_PARAMS = {
    'distance': 5,           # å³°å€¼é—´æœ€å°è·ç¦»
    'prominence': 25,        # å³°å€¼çªå‡ºåº¦
    'height': None           # å³°å€¼é«˜åº¦é˜ˆå€¼ï¼ˆNoneä¸ºè‡ªåŠ¨ï¼‰
}

# è´¨é‡è¯„ä¼°å‚æ•° - å·²ç¦ç”¨ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®
QUALITY_PARAMS = {
    'min_peaks_per_segment': 1,     # æœ€å°‘å³°å€¼æ•°ï¼ˆè®¾ä¸º1ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'max_peaks_per_segment': 1000,  # æœ€å¤šå³°å€¼æ•°ï¼ˆè®¾ä¸ºå¾ˆå¤§å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'gap_threshold_factor': 10.0,   # æ–­ç‚¹æ£€æµ‹é˜ˆå€¼å› å­ï¼ˆè®¾ä¸ºå¾ˆå¤§å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'rr_variability_threshold': 10.0, # RRé—´æœŸå˜å¼‚æ€§é˜ˆå€¼ï¼ˆè®¾ä¸ºå¾ˆå¤§å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'outlier_threshold': 10.0,      # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼ï¼ˆè®¾ä¸ºå¾ˆå¤§å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'rr_range_min_factor': 0.1,     # RRé—´éš”èŒƒå›´ä¸‹é™å› å­ï¼ˆè®¾ä¸ºå¾ˆå°å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'rr_range_max_factor': 10.0,    # RRé—´éš”èŒƒå›´ä¸Šé™å› å­ï¼ˆè®¾ä¸ºå¾ˆå¤§å€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
    'min_segment_quality_score': -1 # æœ€ä½è´¨é‡è¯„åˆ†ï¼ˆè®¾ä¸ºè´Ÿå€¼ï¼Œå–æ¶ˆé™åˆ¶ï¼‰
}

# ============================================================================================
# å¯¼å…¥ä¾èµ–åº“
# ============================================================================================

import os
import sys
import numpy as np
import pandas as pd
from scipy.stats import zscore
from scipy.signal import find_peaks
import argparse
from typing import Tuple, Dict, Optional

class HRVFeatureExtractor:
    """HRVç‰¹å¾æå–å™¨ç±»"""
    
    def __init__(self, segment_duration_ms: int = None):
        """
        åˆå§‹åŒ–HRVç‰¹å¾æå–å™¨
        
        Args:
            segment_duration_ms: æ•°æ®æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®
        """
        # ä½¿ç”¨å…¨å±€é…ç½®å‚æ•°ï¼Œå¦‚æœä¼ å…¥äº†è‡ªå®šä¹‰å‚æ•°åˆ™ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°
        self.segment_duration_ms = segment_duration_ms if segment_duration_ms is not None else SEGMENT_DURATION_MS
        
        # ä½¿ç”¨å…¨å±€å³°å€¼æ£€æµ‹å‚æ•°
        self.peak_detection_params = PEAK_DETECTION_PARAMS.copy()
        
        # ä½¿ç”¨å…¨å±€è´¨é‡è¯„ä¼°å‚æ•°
        self.quality_params = QUALITY_PARAMS.copy()
    
    def detect_peaks(self, data: pd.DataFrame) -> Tuple[Optional[Dict], Optional[Dict]]:
        """
        å¯¹æ•´æ®µæ•°æ®è¿›è¡Œå³°å€¼æ£€æµ‹
        
        Args:
            data: åŒ…å«'æ—¶é—´'å’Œ'æ•°å€¼'åˆ—çš„DataFrame
            
        Returns:
            (peak_info, properties): å³°å€¼ä¿¡æ¯å’Œæ£€æµ‹å±æ€§
        """
        signal_values = data['æ•°å€¼'].values
        
        # æ‰§è¡Œå³°å€¼æ£€æµ‹
        peaks, properties = find_peaks(
            signal_values,
            distance=self.peak_detection_params['distance'],
            prominence=self.peak_detection_params['prominence'],
            height=self.peak_detection_params['height']
        )
        
        if len(peaks) == 0:
            return None, None
        
        # è®¡ç®—å³°å€¼æ—¶é—´å’ŒRRé—´æœŸ
        peak_times = data['æ—¶é—´'].iloc[peaks].values
        rr_intervals = np.diff(peak_times)
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        peak_info = {
            'indices': peaks,
            'times': peak_times,
            'values': signal_values[peaks],
            'rr_intervals': rr_intervals,
            'mean_rr': np.mean(rr_intervals),
            'std_rr': np.std(rr_intervals)
        }
        
        return peak_info, properties
    
    def detect_gaps_and_anomalies(self, peak_info: Dict) -> np.ndarray:
        """
        æ£€æµ‹å³°å€¼é—´çš„æ–­ç‚¹å’Œå¼‚å¸¸
        
        Args:
            peak_info: å³°å€¼ä¿¡æ¯å­—å…¸
            
        Returns:
            anomaly_indices: å¼‚å¸¸ä½ç½®ç´¢å¼•
        """
        if peak_info is None or len(peak_info['rr_intervals']) == 0:
            return np.array([])
        
        rr_intervals = peak_info['rr_intervals']
        mean_rr = peak_info['mean_rr']
        
        # æ–¹æ³•1: åŸºäºé˜ˆå€¼å› å­çš„æ–­ç‚¹æ£€æµ‹
        gap_threshold = mean_rr * self.quality_params['gap_threshold_factor']
        gap_indices = np.where(rr_intervals > gap_threshold)[0]
        
        # æ–¹æ³•2: åŸºäºZ-scoreçš„å¼‚å¸¸æ£€æµ‹
        outlier_indices = np.array([])
        try:
            if len(rr_intervals) > 1 and np.std(rr_intervals) > 1e-10:
                z_scores = np.abs(zscore(rr_intervals))
                valid_z_mask = np.isfinite(z_scores)
                if np.any(valid_z_mask):
                    outlier_indices = np.where((z_scores > self.quality_params['outlier_threshold']) & valid_z_mask)[0]
        except Exception:
            outlier_indices = np.array([])
        
        # åˆå¹¶å¼‚å¸¸ä½ç½®
        if len(gap_indices) > 0 and len(outlier_indices) > 0:
            anomaly_indices = np.unique(np.concatenate([gap_indices, outlier_indices]))
        elif len(gap_indices) > 0:
            anomaly_indices = gap_indices
        elif len(outlier_indices) > 0:
            anomaly_indices = outlier_indices
        else:
            anomaly_indices = np.array([])
        
        return anomaly_indices
    
    def evaluate_segment_quality(self, data_segment: pd.DataFrame, segment_peak_times: np.ndarray, 
                                segment_start_time: float, segment_end_time: float) -> Tuple[bool, Dict]:
        """
        è¯„ä¼°å•ä¸ªæ•°æ®æ®µçš„è´¨é‡
        
        Args:
            data_segment: æ•°æ®æ®µ
            segment_peak_times: æ®µå†…å³°å€¼æ—¶é—´
            segment_start_time: æ®µå¼€å§‹æ—¶é—´
            segment_end_time: æ®µç»“æŸæ—¶é—´
            
        Returns:
            (quality_passed, quality_info): æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥å’Œè´¨é‡ä¿¡æ¯
        """
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å³°å€¼æ—¶é—´
        segment_peaks = segment_peak_times[(segment_peak_times >= segment_start_time) & (segment_peak_times <= segment_end_time)]
        
        quality_score = 0.0
        issues = []
        
        # æ£€æŸ¥1: å³°å€¼æ•°é‡
        peak_count = len(segment_peaks)
        if peak_count < self.quality_params['min_peaks_per_segment']:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å³°å€¼æ•°é‡ä¸è¶³({peak_count}<{self.quality_params['min_peaks_per_segment']})"],
                'rr_mean': 0,
                'rr_std': 0,
                'completeness': 0
            }
        elif peak_count > self.quality_params['max_peaks_per_segment']:
            return False, {
                'quality_score': 0.0,
                'peak_count': peak_count,
                'issues': [f"å³°å€¼æ•°é‡è¿‡å¤š({peak_count}>{self.quality_params['max_peaks_per_segment']})"],
                'rr_mean': 0,
                'rr_std': 0,
                'completeness': 0
            }
        else:
            quality_score += 0.3
        
        # æ£€æŸ¥2: RRé—´æœŸå˜å¼‚æ€§
        if len(segment_peaks) > 1:
            rr_intervals = np.diff(segment_peaks)
            mean_rr = np.mean(rr_intervals)
            std_rr = np.std(rr_intervals)
            cv_rr = std_rr / mean_rr if mean_rr > 0 else float('inf')
            
            if cv_rr > self.quality_params['rr_variability_threshold']:
                return False, {
                    'quality_score': 0.0,
                    'peak_count': peak_count,
                    'issues': [f"RRé—´æœŸå˜å¼‚æ€§è¿‡é«˜(CV={cv_rr:.3f})"],
                    'rr_mean': mean_rr,
                    'rr_std': std_rr,
                    'completeness': 0
                }
            else:
                quality_score += 0.3
            
            # æ£€æŸ¥3: æ–­ç‚¹æ£€æµ‹
            gap_threshold = mean_rr * self.quality_params['gap_threshold_factor']
            gaps = np.sum(rr_intervals > gap_threshold)
            if gaps > 0:
                return False, {
                    'quality_score': 0.0,
                    'peak_count': peak_count,
                    'issues': [f"å­˜åœ¨{gaps}ä¸ªæ–­ç‚¹"],
                    'rr_mean': mean_rr,
                    'rr_std': std_rr,
                    'completeness': 0
                }
            else:
                quality_score += 0.2
            
            # æ£€æŸ¥4: å¼‚å¸¸å€¼æ£€æµ‹
            z_scores = np.abs(zscore(rr_intervals))
            outliers = np.sum(z_scores > self.quality_params['outlier_threshold'])
            if outliers > 0:
                return False, {
                    'quality_score': 0.0,
                    'peak_count': peak_count,
                    'issues': [f"å­˜åœ¨{outliers}ä¸ªå¼‚å¸¸å€¼"],
                    'rr_mean': mean_rr,
                    'rr_std': std_rr,
                    'completeness': 0
                }
            else:
                quality_score += 0.2
            
            # æ£€æŸ¥5: RRé—´éš”èŒƒå›´æ£€æŸ¥
            rr_min_threshold = mean_rr * self.quality_params['rr_range_min_factor']
            rr_max_threshold = mean_rr * self.quality_params['rr_range_max_factor']
            out_of_range_count = np.sum((rr_intervals < rr_min_threshold) | (rr_intervals > rr_max_threshold))
            if out_of_range_count > 0:
                return False, {
                    'quality_score': 0.0,
                    'peak_count': peak_count,
                    'issues': [f"å­˜åœ¨{out_of_range_count}ä¸ªRRé—´éš”è¶…å‡º70%-130%èŒƒå›´"],
                    'rr_mean': mean_rr,
                    'rr_std': std_rr,
                    'completeness': 0
                }
            else:
                quality_score += 0.2
        
        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        expected_points = int(self.segment_duration_ms * len(data_segment) / (data_segment['æ—¶é—´'].max() - data_segment['æ—¶é—´'].min()))
        actual_points = len(data_segment)
        completeness = min(1.0, actual_points / expected_points)
        quality_score += 0.2 * completeness
        
        return True, {
            'quality_score': quality_score,
            'peak_count': peak_count,
            'issues': [],
            'rr_mean': np.mean(rr_intervals) if len(segment_peaks) > 1 else 0,
            'rr_std': np.std(rr_intervals) if len(segment_peaks) > 1 else 0,
            'completeness': completeness
        }
    
    def generate_candidate_segments(self, data: pd.DataFrame, peak_info: Dict) -> list:
        """
        ç”Ÿæˆå€™é€‰æ•°æ®æ®µ
        
        Args:
            data: åŸå§‹æ•°æ®
            peak_info: å³°å€¼ä¿¡æ¯
            
        Returns:
            candidates: å€™é€‰æ•°æ®æ®µåˆ—è¡¨
        """
        if peak_info is None or len(peak_info['times']) < 2:
            return []
        
        # è·å–æ‰€æœ‰å³°å€¼æ—¶é—´
        peak_times = peak_info['times']
        data_start = data['æ—¶é—´'].min()
        data_end = data['æ—¶é—´'].max()
        
        candidates = []
        
        # ä»æ¯ä¸ªå³°å€¼å¼€å§‹ï¼Œå°è¯•ç”Ÿæˆæ•°æ®æ®µ
        for i, start_peak_time in enumerate(peak_times):
            # è®¡ç®—çª—å£ç»“æŸæ—¶é—´
            end_time = start_peak_time + self.segment_duration_ms
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ•°æ®èŒƒå›´
            if end_time > data_end:
                break
            
            # æå–æ•°æ®æ®µ
            mask = (data['æ—¶é—´'] >= start_peak_time) & (data['æ—¶é—´'] <= end_time)
            segment_data = data[mask].copy().reset_index(drop=True)
            
            if len(segment_data) < 50:  # 10ç§’ç‰ˆæœ¬éœ€è¦è¾ƒå°‘çš„æ•°æ®ç‚¹
                continue
            
            # æ‰¾åˆ°è¯¥æ®µå†…çš„å³°å€¼æ—¶é—´
            segment_peak_times = peak_times[(peak_times >= start_peak_time) & (peak_times <= end_time)]
            
            # è¯„ä¼°è´¨é‡
            quality_passed, quality = self.evaluate_segment_quality(segment_data, segment_peak_times, start_peak_time, end_time)
            
            # åªä¿ç•™é€šè¿‡è´¨é‡æ£€æŸ¥çš„æ®µ
            if quality_passed:
                candidates.append({
                    'start_time': start_peak_time,
                    'end_time': end_time,
                    'start_peak_index': i,
                    'data': segment_data,
                    'peak_times': segment_peak_times,
                    'quality': quality
                })
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        candidates.sort(key=lambda x: x['quality']['quality_score'], reverse=True)
        
        # å»é‡ï¼šå¦‚æœä¸¤ä¸ªå€™é€‰æ®µé‡å åº¦å¾ˆé«˜ï¼Œåªä¿ç•™è´¨é‡æ›´é«˜çš„
        filtered_candidates = []
        for candidate in candidates:
            is_duplicate = False
            for existing in filtered_candidates:
                # è®¡ç®—é‡å åº¦
                overlap_start = max(candidate['start_time'], existing['start_time'])
                overlap_end = min(candidate['end_time'], existing['end_time'])
                overlap_duration = max(0, overlap_end - overlap_start)
                overlap_ratio = overlap_duration / self.segment_duration_ms
                
                # å¦‚æœé‡å è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if overlap_ratio > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_candidates.append(candidate)
        
        return filtered_candidates
    
    def calculate_hrv_features(self, rr_ms: np.ndarray) -> Dict:
        """
        è®¡ç®—HRVç‰¹å¾
        
        Args:
            rr_ms: RRé—´æœŸæ•°æ®ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            features: HRVç‰¹å¾å­—å…¸
        """
        if rr_ms is None or len(rr_ms) < 2:
            return {}
        
        feat = {}
        
        # æ—¶åŸŸç‰¹å¾ - å‘é‡åŒ–è®¡ç®—
        rr_diff = np.diff(rr_ms)
        
        # RMSSD - å‘é‡åŒ–è®¡ç®—
        feat['RMSSD'] = float(np.sqrt(np.mean(rr_diff ** 2))) if len(rr_diff) > 0 else np.nan
        # feat['RMSSD'] = float(np.sqrt(np.mean(rr_diff ** 2))) if len(rr_diff) > 0 else np.nan
        # pNN58 - å‘é‡åŒ–è®¡ç®—
        if len(rr_diff) > 0:
            over_58_count = np.sum(np.abs(rr_diff) > 58.0)
            feat['pNN58'] = float((over_58_count / len(rr_diff)) * 100.0)
        else:
            feat['pNN58'] = np.nan
        
        # SDNN - å‘é‡åŒ–è®¡ç®—
        feat['SDNN'] = float(np.std(rr_ms, ddof=1)) if len(rr_ms) > 1 else np.nan
        
        # PoincarÃ©ç‰¹å¾ - å‘é‡åŒ–è®¡ç®—
        if len(rr_diff) > 0:
            var_diff = np.var(rr_diff)
            var_rr = np.var(rr_ms, ddof=1)
            
            sd1 = float(np.sqrt(var_diff / 2.0))
            sd2 = float(np.sqrt(2 * var_rr - var_diff / 2.0))
            
            feat['SD1'] = sd1
            feat['SD2'] = sd2
            feat['SD1_SD2'] = float(sd1 / sd2) if sd2 > 0 else np.nan
        else:
            feat['SD1'] = np.nan
            feat['SD2'] = np.nan
            feat['SD1_SD2'] = np.nan
        
        
        return feat
    
    def extract_features_from_csv(self, csv_path: str, emotion_label: str = "unknown") -> Optional[Dict]:
        """
        ä»CSVæ–‡ä»¶ä¸­æå–HRVç‰¹å¾ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            emotion_label: æƒ…ç»ªæ ‡ç­¾
            
        Returns:
            result: åŒ…å«ç‰¹å¾çš„ç»“æœå­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # è¯»å–CSVæ–‡ä»¶
            data = pd.read_csv(csv_path, header=None)
            
            if data.shape[1] < 2:
                print(f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {os.path.basename(csv_path)} (éœ€è¦è‡³å°‘2åˆ—æ•°æ®)")
                return None
            
            # è®¾ç½®åˆ—å
            data = data.iloc[:, :2].copy()
            data.columns = ['æ—¶é—´', 'æ•°å€¼']
            
            if len(data) < 10:
                print(f"âŒ æ•°æ®ä¸è¶³: {os.path.basename(csv_path)} (éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹)")
                return None
            
            # å³°å€¼æ£€æµ‹
            peak_info, _ = self.detect_peaks(data)
            if peak_info is None or len(peak_info['times']) < 2:
                print(f"âŒ å³°å€¼æ£€æµ‹å¤±è´¥æˆ–å³°å€¼ä¸è¶³: {os.path.basename(csv_path)}")
                return None
            
            # ç›´æ¥ä½¿ç”¨æ‰€æœ‰å³°å€¼è®¡ç®—RRé—´æœŸ
            peak_times = peak_info['times']
            rr_intervals = np.diff(peak_times)
            
            # è®¡ç®—HRVç‰¹å¾
            features = self.calculate_hrv_features(rr_intervals)
            
            if features:
                # æ„å»ºç»“æœï¼ˆåªåŒ…å«6ä¸ªæ ¸å¿ƒHRVç‰¹å¾ï¼‰
                result = {
                    'RMSSD': features.get('RMSSD', np.nan),
                    'pNN58': features.get('pNN58', np.nan),
                    'SDNN': features.get('SDNN', np.nan),
                    'SD1': features.get('SD1', np.nan),
                    'SD2': features.get('SD2', np.nan),
                    'SD1_SD2': features.get('SD1_SD2', np.nan),
                    'emotion': emotion_label,
                    'peak_count': len(peak_times),
                    'segment_duration': self.segment_duration_ms / 1000.0
                }
                return result
            
            print(f"âŒ HRVç‰¹å¾è®¡ç®—å¤±è´¥: {os.path.basename(csv_path)}")
            return None
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {os.path.basename(csv_path)} -> {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä»CSVæ–‡ä»¶ä¸­æå–HRVç‰¹å¾')
    parser.add_argument('input_csv', nargs='?', default=INPUT_CSV_PATH, help=f'è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š{INPUT_CSV_PATH}ï¼‰')
    parser.add_argument('-o', '--output', default=OUTPUT_CSV_PATH, help=f'è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š{OUTPUT_CSV_PATH}ï¼‰')
    parser.add_argument('-e', '--emotion', default=EMOTION_LABEL, help=f'æƒ…ç»ªæ ‡ç­¾ï¼ˆé»˜è®¤ï¼š{EMOTION_LABEL}ï¼‰')
    parser.add_argument('-d', '--duration', type=int, default=SEGMENT_DURATION_MS, help=f'æ•°æ®æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ï¼š{SEGMENT_DURATION_MS}ï¼‰')
    parser.add_argument('--use-config', action='store_true', help='ä½¿ç”¨ä»£ç é¡¶éƒ¨çš„é…ç½®å‚æ•°ï¼Œå¿½ç•¥å‘½ä»¤è¡Œå‚æ•°')
    
    args = parser.parse_args()
    
    # å¦‚æœä½¿ç”¨é…ç½®æ¨¡å¼ï¼Œä½¿ç”¨å…¨å±€é…ç½®å‚æ•°
    if args.use_config:
        input_csv = INPUT_CSV_PATH
        output_csv = OUTPUT_CSV_PATH
        emotion = EMOTION_LABEL
        duration = SEGMENT_DURATION_MS
        print("ğŸ”§ ä½¿ç”¨ä»£ç é¡¶éƒ¨çš„é…ç½®å‚æ•°")
    else:
        input_csv = args.input_csv
        output_csv = args.output
        emotion = args.emotion
        duration = args.duration
        print("ğŸ”§ ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_csv):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_csv}")
        print(f"ğŸ’¡ æç¤ºï¼šè¯·ä¿®æ”¹ä»£ç é¡¶éƒ¨çš„ INPUT_CSV_PATH å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --use-config å‚æ•°")
        sys.exit(1)
    
    print(f"ğŸ” å¼€å§‹å¤„ç†æ–‡ä»¶: {input_csv}")
    print(f"ğŸ·ï¸ æƒ…ç»ªæ ‡ç­¾: {emotion}")
    print(f"â±ï¸ æ•°æ®æ®µé•¿åº¦: {duration/1000:.1f} ç§’")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = HRVFeatureExtractor(segment_duration_ms=duration)
    
    # æå–ç‰¹å¾
    result = extractor.extract_features_from_csv(input_csv, emotion)
    
    if result is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        sys.exit(1)
    
    # æ˜¾ç¤ºç»“æœ
    print("\nâœ… ç‰¹å¾æå–æˆåŠŸï¼")
    print(f"ğŸ“Š æå–çš„6ä¸ªHRVç‰¹å¾:")
    feature_names = ['RMSSD', 'pNN58', 'SDNN', 'SD1', 'SD2', 'SD1_SD2']
    for name in feature_names:
        value = result[name]
        if np.isnan(value):
            print(f"   {name}: NaN")
        else:
            print(f"   {name}: {value:.4f}")
    
    print(f"\nğŸ“ˆ å¤„ç†ä¿¡æ¯:")
    print(f"   å³°å€¼æ•°é‡: {result['peak_count']}")
    print(f"   æ•°æ®æ®µé•¿åº¦: {result['segment_duration']:.1f} ç§’")
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    if output_csv:
        # åˆ›å»ºDataFrame
        df = pd.DataFrame([result])
        
        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—ï¼ˆåªåŒ…å«6ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
        cols = ['RMSSD', 'pNN58', 'SDNN', 'SD1', 'SD2', 'SD1_SD2']
        df = df[cols]
        
        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆä¸åŒ…å«è¡¨å¤´å’Œç´¢å¼•ï¼‰
        df.to_csv(output_csv, index=False, header=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
    
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
